{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/shyel/repositories/mamba_linear_code\")\n",
    "os.chdir(\"/home/shyel/repositories/mamba_linear_code\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDPC_N49_K24.alist\n"
     ]
    }
   ],
   "source": [
    "from configuration import Code, Config\n",
    "from dataset import get_generator_and_parity\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def code_from_hint(hint,):\n",
    "    code_files = os.listdir(CODES_PATH)\n",
    "    code_files = [f for f in code_files if hint in f][0]\n",
    "    print(code_files)\n",
    "    code_n = int(code_files.split('_')[1][1:])\n",
    "    code_k = int(code_files.split('_')[-1][1:].split('.')[0])\n",
    "    code_type = code_files.split('_')[0]\n",
    "    code = Code(code_n, code_k, code_type)\n",
    "    return code\n",
    "\n",
    "OUTPUT_PATH = \".semi-working-mamba-mask-large-bidir/.output-8/\"\n",
    "CODES_PATH = \"codes/\"\n",
    "example_code = code_from_hint(\"LDPC_N49_K24\")\n",
    "G,H = get_generator_and_parity(example_code, standard_form=True)\n",
    "example_code.generator_matrix = torch.from_numpy(G).transpose(0,1).long()\n",
    "example_code.pc_matrix = torch.from_numpy(H).long()\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "config = Config(\n",
    "    code=example_code,\n",
    "    d_model=128, # example_code.n + H.shape[0],\n",
    "    path=OUTPUT_PATH,\n",
    "    N_dec=8,\n",
    "    warmup_lr=1.0e-3,\n",
    "    lr=5e-4,\n",
    "    epochs=1000,\n",
    "    eta_min=1e-12,\n",
    "    batch_size=64,\n",
    "    gradient_clipping=1.0\n",
    ")\n",
    "\n",
    "handlers = [\n",
    "        logging.FileHandler(os.path.join(OUTPUT_PATH, 'logging.txt')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s',\n",
    "                    handlers=handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyel/repositories/mamba_linear_code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import Mamba\n",
    "from dataset import EbN0_to_std, ECC_Dataset, train, test, sign_to_bin, FER, BER, bin_to_sign\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, LayerNorm\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "\n",
    "try:\n",
    "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
    "except ImportError:\n",
    "    causal_conv1d_fn, causal_conv1d_update = None, None\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "except ImportError:\n",
    "    selective_state_update = None\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
    "except ImportError:\n",
    "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
    "\n",
    "# ECCM\n",
    "from configuration import Code, Config\n",
    "import numpy as np\n",
    "\n",
    "from dataset import sign_to_bin, bin_to_sign\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "import copy\n",
    "\n",
    "def build_mask(code):\n",
    "    mask_size = code.n + code.pc_matrix.size(0)\n",
    "    mask = torch.eye(mask_size, mask_size)\n",
    "    for ii in range(code.pc_matrix.size(0)):\n",
    "        idx = torch.where(code.pc_matrix[ii] > 0)[0]\n",
    "        for jj in idx:\n",
    "            for kk in idx:\n",
    "                if jj != kk:\n",
    "                    mask[jj, kk] += 1\n",
    "                    mask[kk, jj] += 1\n",
    "                    mask[code.n + ii, jj] += 1\n",
    "                    mask[jj, code.n + ii] += 1\n",
    "    src_mask = ~ (mask > 0)\n",
    "    return src_mask\n",
    "\n",
    "def mask_larger_matrix(M, mask):\n",
    "    M[:, :mask.size(0), :mask.size(1)][mask.expand(M.size(0),mask.size(0),mask.size(1))] = 0.0\n",
    "    M[:, mask.size(0):, mask.size(1):] = 0\n",
    "    return M\n",
    "\n",
    "def mask(A,B,C,D, mask):\n",
    "    B = mask_larger_matrix(B, mask)\n",
    "    C = mask_larger_matrix(C, mask)\n",
    "    return A, B, C, D\n",
    "\n",
    "def mamba_inner_fn(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    pc_mask, A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    assert causal_conv1d_fn is not None, \"causal_conv1d_fn is not available. Please install causal-conv1d.\"\n",
    "    L = xz.shape[-1]\n",
    "    delta_rank = delta_proj_weight.shape[1]\n",
    "    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "    x, z = xz.chunk(2, dim=1)\n",
    "    x = causal_conv1d_fn(x, rearrange(conv1d_weight, \"d 1 w -> d w\"), conv1d_bias, activation=\"silu\")\n",
    "    # We're being very careful here about the layout, to avoid extra transposes.\n",
    "    # We want delta to have d as the slowest moving dimension\n",
    "    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "    x_dbl = F.linear(rearrange(x, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()\n",
    "    delta = rearrange(delta, \"d (b l) -> b d l\", l=L)\n",
    "    if B is None:  # variable B\n",
    "        B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl d)\n",
    "        if B_proj_bias is not None:\n",
    "            B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "        if not A.is_complex():\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            B = rearrange(B, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    if C is None:  # variable B\n",
    "        C = x_dbl[:, -d_state:]  # (bl d)\n",
    "        if C_proj_bias is not None:\n",
    "            C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "        if not A.is_complex():\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            C = rearrange(C, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    # mask\n",
    "    A,B,C,D = mask(A,B,C,D,pc_mask)\n",
    "    y = selective_scan_fn(x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True)\n",
    "    mask_larger_matrix(y, pc_mask)\n",
    "    return F.linear(rearrange(y, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n",
    "\n",
    "\n",
    "class ECCMLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        code: Code,\n",
    "        d_model=16,\n",
    "        d_conv=4,\n",
    "        expand=1,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        use_fast_path=True,  # Fused kernel options\n",
    "        layer_idx=None,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.code_length = code.n\n",
    "        self.output_length = self.code_length + code.pc_matrix.size(0)\n",
    "        d_state = int(2**np.ceil(np.log2(self.output_length)))\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert self.d_model * expand >= self.output_length, f\"d_model * expand {self.d_model * expand} must be larger than the output length, (code_length + syndrome length).\"\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "        self.use_fast_path = use_fast_path\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            groups=self.d_inner,\n",
    "            padding=d_conv - 1,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "\n",
    "        self.activation = \"silu\"\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = nn.Linear(\n",
    "            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs\n",
    "        )\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        self.dt_proj.bias._no_reinit = True\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=self.d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        self.A_log = nn.Parameter(A_log)\n",
    "        self.A_log._no_weight_decay = True\n",
    "\n",
    "        # D \"skip\" parameter\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32\n",
    "        self.D._no_weight_decay = True\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "\n",
    "    def forward(self, hidden_states, pc_mask, inference_params=None):\n",
    "        \"\"\"\n",
    "        hidden_states: (B, L, D)\n",
    "        pc_mask: (L, P)\n",
    "        Returns: same shape as hidden_states\n",
    "        \"\"\"\n",
    "        batch, seqlen, dim = hidden_states.shape\n",
    "\n",
    "        conv_state, ssm_state = None, None\n",
    "        if inference_params is not None:\n",
    "            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n",
    "            if inference_params.seqlen_offset > 0:\n",
    "                # The states are updated inplace\n",
    "                out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n",
    "                return out\n",
    "\n",
    "        # We do matmul and transpose BLH -> HBL at the same time\n",
    "        xz = rearrange(\n",
    "            self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n",
    "            \"d (b l) -> b d l\",\n",
    "            l=seqlen,\n",
    "        )\n",
    "        if self.in_proj.bias is not None:\n",
    "            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\")\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n",
    "        # In the backward pass we write dx and dz next to each other to avoid torch.cat\n",
    "        if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:  # Doesn't support outputting the states\n",
    "            out = mamba_inner_fn(\n",
    "                xz,\n",
    "                self.conv1d.weight,\n",
    "                self.conv1d.bias,\n",
    "                self.x_proj.weight,\n",
    "                self.dt_proj.weight,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "                pc_mask,\n",
    "                A,\n",
    "                None,  # input-dependent B\n",
    "                None,  # input-dependent C\n",
    "                self.D.float(),\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "            )\n",
    "        else:\n",
    "            x, z = xz.chunk(2, dim=1)\n",
    "            # Compute short convolution\n",
    "            if conv_state is not None:\n",
    "                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
    "                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
    "                conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))  # Update state (B D W)\n",
    "            if causal_conv1d_fn is None:\n",
    "                x = self.act(self.conv1d(x)[..., :seqlen])\n",
    "            else:\n",
    "                assert self.activation in [\"silu\", \"swish\"]\n",
    "                x = causal_conv1d_fn(\n",
    "                    x=x,\n",
    "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
    "                    bias=self.conv1d.bias,\n",
    "                    activation=self.activation,\n",
    "                )\n",
    "\n",
    "            # We're careful here about the layout, to avoid extra transposes.\n",
    "            # We want dt to have d as the slowest moving dimension\n",
    "            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "            x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))  # (bl d)\n",
    "            dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "            dt = self.dt_proj.weight @ dt.t()\n",
    "            dt = rearrange(dt, \"d (b l) -> b d l\", l=seqlen)\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            assert self.activation in [\"silu\", \"swish\"]\n",
    "            D = self.D.float()\n",
    "            A,B,C,D = mask(A,B,C,D, pc_mask)\n",
    "            \n",
    "            y = selective_scan_fn(\n",
    "                x,\n",
    "                dt,\n",
    "                A,\n",
    "                B,\n",
    "                C,\n",
    "                D,\n",
    "                z=z,\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "                return_last_state=ssm_state is not None,\n",
    "            )\n",
    "            if ssm_state is not None:\n",
    "                y, last_state = y\n",
    "                ssm_state.copy_(last_state)\n",
    "            y = rearrange(y, \"b d l -> b l d\")\n",
    "            mask_larger_matrix(y, pc_mask)\n",
    "            out = self.out_proj(y)\n",
    "        return out\n",
    "\n",
    "    def step(self, hidden_states, conv_state, ssm_state, pc_mask):\n",
    "        dtype = hidden_states.dtype\n",
    "        assert hidden_states.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
    "        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)\n",
    "        x, z = xz.chunk(2, dim=-1)  # (B D)\n",
    "\n",
    "        # Conv step\n",
    "        if causal_conv1d_update is None:\n",
    "            conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)\n",
    "            conv_state[:, :, -1] = x\n",
    "            x = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
    "            if self.conv1d.bias is not None:\n",
    "                x = x + self.conv1d.bias\n",
    "            x = self.act(x).to(dtype=dtype)\n",
    "        else:\n",
    "            x = causal_conv1d_update(\n",
    "                x,\n",
    "                conv_state,\n",
    "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
    "                self.conv1d.bias,\n",
    "                self.activation,\n",
    "            )\n",
    "\n",
    "        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)\n",
    "        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        # Don't add dt_bias here\n",
    "        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)\n",
    "        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n",
    "\n",
    "        D = self.D.float()\n",
    "        A,B,C,D = mask(A,B,C,D,pc_mask)\n",
    "            \n",
    "        # SSM step\n",
    "        if selective_state_update is None:\n",
    "            # Discretize A and B\n",
    "            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))\n",
    "            dA = torch.exp(torch.einsum(\"bd,dn->bdn\", dt, A))\n",
    "            dB = torch.einsum(\"bd,bn->bdn\", dt, B)\n",
    "            ssm_state.copy_(ssm_state * dA + rearrange(x, \"b d -> b d 1\") * dB)\n",
    "            y = torch.einsum(\"bdn,bn->bd\", ssm_state.to(dtype), C)\n",
    "            y = y + D.to(dtype) * x\n",
    "            y = y * self.act(z)  # (B D)\n",
    "        else:\n",
    "            y = selective_state_update(\n",
    "                ssm_state, x, dt, A, B, C, D, z=z, dt_bias=self.dt_proj.bias, dt_softplus=True\n",
    "            )\n",
    "\n",
    "        mask_larger_matrix(y, pc_mask)\n",
    "        out = self.out_proj(y)\n",
    "        return out.unsqueeze(1), conv_state, ssm_state\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        device = self.out_proj.weight.device\n",
    "        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype\n",
    "        conv_state = torch.zeros(\n",
    "            batch_size, self.d_model * self.expand, self.d_conv, device=device, dtype=conv_dtype\n",
    "        )\n",
    "        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype\n",
    "        # ssm_dtype = torch.float32\n",
    "        ssm_state = torch.zeros(\n",
    "            batch_size, self.d_model * self.expand, self.d_state, device=device, dtype=ssm_dtype\n",
    "        )\n",
    "        return conv_state, ssm_state\n",
    "\n",
    "    def _get_states_from_cache(self, inference_params, batch_size, initialize_states=False):\n",
    "        assert self.layer_idx is not None\n",
    "        if self.layer_idx not in inference_params.key_value_memory_dict:\n",
    "            batch_shape = (batch_size,)\n",
    "            conv_state = torch.zeros(\n",
    "                batch_size,\n",
    "                self.d_model * self.expand,\n",
    "                self.d_conv,\n",
    "                device=self.conv1d.weight.device,\n",
    "                dtype=self.conv1d.weight.dtype,\n",
    "            )\n",
    "            ssm_state = torch.zeros(\n",
    "                batch_size,\n",
    "                self.d_model * self.expand,\n",
    "                self.d_state,\n",
    "                device=self.dt_proj.weight.device,\n",
    "                dtype=self.dt_proj.weight.dtype,\n",
    "                # dtype=torch.float32,\n",
    "            )\n",
    "            inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)\n",
    "        else:\n",
    "            conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
    "            # TODO: What if batch size changes between generation, and we reuse the same states?\n",
    "            if initialize_states:\n",
    "                conv_state.zero_()\n",
    "                ssm_state.zero_()\n",
    "        return conv_state, ssm_state\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def build_mask(code):\n",
    "    mask_size = code.n + code.pc_matrix.size(0)\n",
    "    mask = torch.eye(mask_size, mask_size)\n",
    "    for ii in range(code.pc_matrix.size(0)):\n",
    "        idx = torch.where(code.pc_matrix[ii] > 0)[0]\n",
    "        for jj in idx:\n",
    "            for kk in idx:\n",
    "                if jj != kk:\n",
    "                    mask[jj, kk] += 1\n",
    "                    mask[kk, jj] += 1\n",
    "                    mask[code.n + ii, jj] += 1\n",
    "                    mask[jj, code.n + ii] += 1\n",
    "    src_mask = ~ (mask > 0)\n",
    "    return src_mask\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.syndrom_length = config.code.pc_matrix.size(0)\n",
    "        self.n = config.code.n\n",
    "        self.register_buffer('pc_mask', build_mask(config.code))\n",
    "        self.mamba = ECCMLayer(\n",
    "            code=config.code,\n",
    "            d_model=config.d_model,\n",
    "        )\n",
    "        self.src_embed = torch.nn.Parameter(torch.ones(\n",
    "            (self.n + self.syndrom_length, config.d_model)))\n",
    "        # self.up_output_dim = torch.nn.Linear(config.d_model, self.n + self.syndrom_length)\n",
    "        # self.resize_output_dim = torch.nn.Linear(self.n + self.syndrom_length, 1)\n",
    "        # self.norm_output = LayerNorm((self.n,))\n",
    "        self.resize_output_dim = torch.nn.Linear(config.d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb: torch.Tensor = self.src_embed.unsqueeze(0) * x\n",
    "        mamba_forward_out = self.mamba.forward(emb, self.pc_mask)\n",
    "        mamba_reverse_out = self.mamba.forward(torch.flip(emb,[1]), torch.flip(self.pc_mask, [1]))\n",
    "        mamba_out = mamba_forward_out + torch.flip(mamba_reverse_out, [1])\n",
    "        # up_convert_out: torch.Tensor = self.up_output_dim(mamba_out)\n",
    "        # gated_output = up_convert_out.masked_fill(self.pc_mask.bool(), 0.0)\n",
    "        # out_dim_corrected: torch.Tensor = self.resize_output_dim(gated_output).squeeze(0)\n",
    "        out_dim_corrected = self.resize_output_dim(mamba_out)\n",
    "        return F.tanh(out_dim_corrected)\n",
    "\n",
    "class ECCM(torch.nn.Module):\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.n = config.code.n\n",
    "        self.syndrom_length = config.code.pc_matrix.size(0)\n",
    "        self.mamba: ModuleList = clones(EncoderLayer(config,), config.N_dec)\n",
    "        self.resize_output_length = torch.nn.Linear(self.n + self.syndrom_length, self.n)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, magnitude, syndrome):\n",
    "        inp = torch.cat([magnitude, syndrome], -1)\n",
    "        emb = inp.unsqueeze(-1)\n",
    "        hidden = emb\n",
    "        for sublayer in self.mamba:\n",
    "            hidden: torch.Tensor = sublayer.forward(hidden)\n",
    "        hidden = emb + hidden\n",
    "        out = self.resize_output_length(hidden.squeeze(-1))\n",
    "        return (1-F.tanh(out))/2\n",
    "\n",
    "    def loss(self, z_pred, z2, y):\n",
    "        loss = F.binary_cross_entropy(\n",
    "            z_pred, sign_to_bin(torch.sign(z2)))\n",
    "        x_pred = sign_to_bin(torch.sign(bin_to_sign(F.tanh(z_pred)) * torch.sign(y)))\n",
    "        return loss, x_pred\n",
    "\n",
    "\n",
    "model = ECCM(config=config).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.56it/s]Training epoch 1, Batch 1000/1000: LR=1.00e-03, Loss=1.88e-01 BER=5.82e-02 FER=7.50e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.24it/s]\n",
      "Epoch 1 Train Time 25.487072467803955s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.32it/s]Training epoch 2, Batch 1000/1000: LR=1.00e-03, Loss=1.06e-01 BER=3.83e-02 FER=4.78e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 2 Train Time 25.382628440856934s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.60it/s]Training epoch 3, Batch 1000/1000: LR=1.00e-03, Loss=8.99e-02 BER=3.36e-02 FER=4.34e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 3 Train Time 25.28503441810608s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.38it/s]Training epoch 4, Batch 1000/1000: LR=1.00e-03, Loss=8.41e-02 BER=3.20e-02 FER=4.17e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 4 Train Time 25.258424997329712s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.41it/s]Training epoch 5, Batch 1000/1000: LR=1.00e-03, Loss=8.26e-02 BER=3.17e-02 FER=4.15e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 5 Train Time 25.2619206905365s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.45it/s]Training epoch 6, Batch 1000/1000: LR=1.00e-03, Loss=8.19e-02 BER=3.14e-02 FER=4.11e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 6 Train Time 25.296700477600098s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.13it/s]Training epoch 7, Batch 1000/1000: LR=1.00e-03, Loss=8.11e-02 BER=3.12e-02 FER=4.09e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 7 Train Time 25.304100275039673s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.50it/s]Training epoch 8, Batch 1000/1000: LR=1.00e-03, Loss=8.08e-02 BER=3.10e-02 FER=4.08e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 8 Train Time 25.283207654953003s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.49it/s]Training epoch 9, Batch 1000/1000: LR=1.00e-03, Loss=7.97e-02 BER=3.07e-02 FER=4.04e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 9 Train Time 25.259868383407593s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Test EbN0=0, BER=1.55e-01 -ln(BER)=1.86e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Test EbN0=1, BER=1.29e-01 -ln(BER)=2.04e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.91it/s]\n",
      "Test EbN0=2, BER=8.44e-02 -ln(BER)=2.47e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n",
      "Test EbN0=3, BER=5.65e-02 -ln(BER)=2.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Test EbN0=4, BER=2.85e-02 -ln(BER)=3.56e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "Test EbN0=5, BER=1.13e-02 -ln(BER)=4.48e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=6, BER=2.59e-03 -ln(BER)=5.96e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=7, BER=7.57e-04 -ln(BER)=7.19e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Test EbN0=8, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n",
      "/tmp/ipykernel_480425/1694856526.py:47: RuntimeWarning: divide by zero encountered in log\n",
      "  ln_ber = -np.log(test_ber)\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.33it/s]Training epoch 10, Batch 1000/1000: LR=5.00e-04, Loss=7.80e-02 BER=3.00e-02 FER=3.97e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 10 Train Time 25.265625s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
      "Test EbN0=0, BER=1.53e-01 -ln(BER)=1.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Test EbN0=1, BER=1.21e-01 -ln(BER)=2.11e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Test EbN0=2, BER=8.77e-02 -ln(BER)=2.43e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n",
      "Test EbN0=3, BER=5.53e-02 -ln(BER)=2.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Test EbN0=4, BER=2.74e-02 -ln(BER)=3.60e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Test EbN0=5, BER=7.57e-03 -ln(BER)=4.88e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Test EbN0=6, BER=1.91e-03 -ln(BER)=6.26e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]\n",
      "Test EbN0=7, BER=2.79e-04 -ln(BER)=8.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n",
      "Test EbN0=8, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.54it/s]Training epoch 11, Batch 1000/1000: LR=5.00e-04, Loss=7.69e-02 BER=2.96e-02 FER=3.97e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 11 Train Time 25.315830945968628s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.47it/s]Training epoch 12, Batch 1000/1000: LR=5.00e-04, Loss=7.64e-02 BER=2.93e-02 FER=3.92e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 12 Train Time 25.322864294052124s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.54it/s]Training epoch 13, Batch 1000/1000: LR=5.00e-04, Loss=7.52e-02 BER=2.89e-02 FER=3.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 13 Train Time 25.3273823261261s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.48it/s]Training epoch 14, Batch 1000/1000: LR=5.00e-04, Loss=7.46e-02 BER=2.88e-02 FER=3.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.64it/s]\n",
      "Epoch 14 Train Time 25.229196786880493s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.20it/s]Training epoch 15, Batch 1000/1000: LR=5.00e-04, Loss=7.32e-02 BER=2.79e-02 FER=3.75e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 15 Train Time 25.35834527015686s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.10it/s]Training epoch 16, Batch 1000/1000: LR=5.00e-04, Loss=7.17e-02 BER=2.74e-02 FER=3.74e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 16 Train Time 25.28838086128235s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.38it/s]Training epoch 17, Batch 1000/1000: LR=5.00e-04, Loss=7.14e-02 BER=2.72e-02 FER=3.71e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 17 Train Time 25.288267850875854s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.72it/s]Training epoch 18, Batch 1000/1000: LR=5.00e-04, Loss=7.12e-02 BER=2.71e-02 FER=3.70e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.60it/s]\n",
      "Epoch 18 Train Time 25.25286078453064s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.28it/s]Training epoch 19, Batch 1000/1000: LR=5.00e-04, Loss=7.05e-02 BER=2.69e-02 FER=3.66e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 19 Train Time 25.311847448349s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.31it/s]Training epoch 20, Batch 1000/1000: LR=5.00e-04, Loss=6.98e-02 BER=2.65e-02 FER=3.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 20 Train Time 25.295361042022705s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Test EbN0=0, BER=1.51e-01 -ln(BER)=1.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Test EbN0=1, BER=1.20e-01 -ln(BER)=2.12e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "Test EbN0=2, BER=8.22e-02 -ln(BER)=2.50e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.09it/s]\n",
      "Test EbN0=3, BER=4.87e-02 -ln(BER)=3.02e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=4, BER=2.18e-02 -ln(BER)=3.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Test EbN0=5, BER=8.73e-03 -ln(BER)=4.74e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n",
      "Test EbN0=6, BER=1.28e-03 -ln(BER)=6.66e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Test EbN0=7, BER=1.99e-04 -ln(BER)=8.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.03it/s]Training epoch 21, Batch 1000/1000: LR=5.00e-04, Loss=6.94e-02 BER=2.63e-02 FER=3.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 21 Train Time 25.311874628067017s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.37it/s]Training epoch 22, Batch 1000/1000: LR=5.00e-04, Loss=6.87e-02 BER=2.61e-02 FER=3.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 22 Train Time 25.301144123077393s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.56it/s]Training epoch 23, Batch 1000/1000: LR=5.00e-04, Loss=6.85e-02 BER=2.58e-02 FER=3.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 23 Train Time 25.277297735214233s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.41it/s]Training epoch 24, Batch 1000/1000: LR=5.00e-04, Loss=6.80e-02 BER=2.58e-02 FER=3.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 24 Train Time 25.379584312438965s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.36it/s]Training epoch 25, Batch 1000/1000: LR=5.00e-04, Loss=6.78e-02 BER=2.56e-02 FER=3.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 25 Train Time 25.30775260925293s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.38it/s]Training epoch 26, Batch 1000/1000: LR=5.00e-04, Loss=6.72e-02 BER=2.53e-02 FER=3.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.39it/s]\n",
      "Epoch 26 Train Time 25.38873028755188s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.41it/s]Training epoch 27, Batch 1000/1000: LR=5.00e-04, Loss=6.74e-02 BER=2.54e-02 FER=3.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 27 Train Time 25.322981119155884s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.52it/s]Training epoch 28, Batch 1000/1000: LR=5.00e-04, Loss=6.70e-02 BER=2.52e-02 FER=3.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 28 Train Time 25.26067876815796s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.37it/s]Training epoch 29, Batch 1000/1000: LR=5.00e-04, Loss=6.72e-02 BER=2.53e-02 FER=3.50e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 29 Train Time 25.35400152206421s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 30, Batch 1000/1000: LR=5.00e-04, Loss=6.67e-02 BER=2.51e-02 FER=3.47e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 30 Train Time 25.331239223480225s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Test EbN0=0, BER=1.50e-01 -ln(BER)=1.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Test EbN0=1, BER=1.12e-01 -ln(BER)=2.19e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Test EbN0=2, BER=7.96e-02 -ln(BER)=2.53e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Test EbN0=3, BER=4.50e-02 -ln(BER)=3.10e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.05it/s]\n",
      "Test EbN0=4, BER=1.87e-02 -ln(BER)=3.98e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Test EbN0=5, BER=7.02e-03 -ln(BER)=4.96e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Test EbN0=6, BER=1.08e-03 -ln(BER)=6.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
      "Test EbN0=7, BER=1.59e-04 -ln(BER)=8.74e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.41it/s]Training epoch 31, Batch 1000/1000: LR=4.99e-04, Loss=6.65e-02 BER=2.51e-02 FER=3.46e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 31 Train Time 25.278353929519653s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.41it/s]Training epoch 32, Batch 1000/1000: LR=4.99e-04, Loss=6.63e-02 BER=2.50e-02 FER=3.46e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 32 Train Time 25.28509020805359s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.04it/s]Training epoch 33, Batch 1000/1000: LR=4.99e-04, Loss=6.62e-02 BER=2.48e-02 FER=3.43e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 33 Train Time 25.267104148864746s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.50it/s]Training epoch 34, Batch 1000/1000: LR=4.99e-04, Loss=6.62e-02 BER=2.49e-02 FER=3.43e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.64it/s]\n",
      "Epoch 34 Train Time 25.22980546951294s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.48it/s]Training epoch 35, Batch 1000/1000: LR=4.99e-04, Loss=6.65e-02 BER=2.51e-02 FER=3.43e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 35 Train Time 25.286873817443848s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.20it/s]Training epoch 36, Batch 1000/1000: LR=4.99e-04, Loss=6.62e-02 BER=2.48e-02 FER=3.43e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.36it/s]\n",
      "Epoch 36 Train Time 25.408246755599976s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.40it/s]Training epoch 37, Batch 1000/1000: LR=4.99e-04, Loss=6.60e-02 BER=2.50e-02 FER=3.41e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 37 Train Time 25.34088921546936s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 38, Batch 1000/1000: LR=4.99e-04, Loss=6.55e-02 BER=2.46e-02 FER=3.39e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 38 Train Time 25.26522183418274s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.58it/s]Training epoch 39, Batch 1000/1000: LR=4.99e-04, Loss=6.57e-02 BER=2.47e-02 FER=3.39e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 39 Train Time 25.297812461853027s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.49it/s]Training epoch 40, Batch 1000/1000: LR=4.99e-04, Loss=6.51e-02 BER=2.45e-02 FER=3.37e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 40 Train Time 25.292587995529175s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Test EbN0=0, BER=1.52e-01 -ln(BER)=1.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n",
      "Test EbN0=1, BER=1.19e-01 -ln(BER)=2.13e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n",
      "Test EbN0=2, BER=7.74e-02 -ln(BER)=2.56e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Test EbN0=3, BER=4.50e-02 -ln(BER)=3.10e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Test EbN0=4, BER=1.83e-02 -ln(BER)=4.00e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Test EbN0=5, BER=5.50e-03 -ln(BER)=5.20e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Test EbN0=6, BER=7.17e-04 -ln(BER)=7.24e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Test EbN0=7, BER=1.59e-04 -ln(BER)=8.74e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.47it/s]Training epoch 41, Batch 1000/1000: LR=4.99e-04, Loss=6.46e-02 BER=2.43e-02 FER=3.35e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 41 Train Time 25.275410175323486s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.59it/s]Training epoch 42, Batch 1000/1000: LR=4.99e-04, Loss=6.47e-02 BER=2.43e-02 FER=3.32e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 42 Train Time 25.279992818832397s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.45it/s]Training epoch 43, Batch 1000/1000: LR=4.99e-04, Loss=6.51e-02 BER=2.45e-02 FER=3.35e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 43 Train Time 25.298361778259277s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 44, Batch 1000/1000: LR=4.99e-04, Loss=6.44e-02 BER=2.43e-02 FER=3.33e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 44 Train Time 25.34769916534424s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.87it/s]Training epoch 45, Batch 1000/1000: LR=4.98e-04, Loss=6.45e-02 BER=2.42e-02 FER=3.29e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 45 Train Time 25.330732583999634s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.15it/s]Training epoch 46, Batch 1000/1000: LR=4.98e-04, Loss=6.41e-02 BER=2.42e-02 FER=3.29e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 46 Train Time 25.289458751678467s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.56it/s]Training epoch 47, Batch 1000/1000: LR=4.98e-04, Loss=6.40e-02 BER=2.40e-02 FER=3.26e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 47 Train Time 25.277543783187866s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.48it/s]Training epoch 48, Batch 1000/1000: LR=4.98e-04, Loss=6.41e-02 BER=2.41e-02 FER=3.27e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 48 Train Time 25.32797384262085s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.51it/s]Training epoch 49, Batch 1000/1000: LR=4.98e-04, Loss=6.35e-02 BER=2.39e-02 FER=3.26e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 49 Train Time 25.295298099517822s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.10it/s]Training epoch 50, Batch 1000/1000: LR=4.98e-04, Loss=6.39e-02 BER=2.41e-02 FER=3.25e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.60it/s]\n",
      "Epoch 50 Train Time 25.256234645843506s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Test EbN0=0, BER=1.56e-01 -ln(BER)=1.86e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n",
      "Test EbN0=1, BER=1.18e-01 -ln(BER)=2.13e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Test EbN0=2, BER=7.82e-02 -ln(BER)=2.55e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Test EbN0=3, BER=4.03e-02 -ln(BER)=3.21e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Test EbN0=4, BER=2.30e-02 -ln(BER)=3.77e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Test EbN0=5, BER=6.22e-03 -ln(BER)=5.08e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Test EbN0=6, BER=1.12e-03 -ln(BER)=6.80e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n",
      "Test EbN0=7, BER=1.20e-04 -ln(BER)=9.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 51, Batch 1000/1000: LR=4.98e-04, Loss=6.29e-02 BER=2.36e-02 FER=3.20e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 51 Train Time 25.32319974899292s\n",
      "\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.43it/s]Training epoch 52, Batch 1000/1000: LR=4.98e-04, Loss=6.31e-02 BER=2.38e-02 FER=3.23e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 52 Train Time 25.342986345291138s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.56it/s]Training epoch 53, Batch 1000/1000: LR=4.98e-04, Loss=6.35e-02 BER=2.39e-02 FER=3.21e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 53 Train Time 25.259953022003174s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.32it/s]Training epoch 54, Batch 1000/1000: LR=4.98e-04, Loss=6.28e-02 BER=2.37e-02 FER=3.18e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 54 Train Time 25.312493562698364s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.32it/s]Training epoch 55, Batch 1000/1000: LR=4.98e-04, Loss=6.24e-02 BER=2.35e-02 FER=3.16e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 55 Train Time 25.293985605239868s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.37it/s]Training epoch 56, Batch 1000/1000: LR=4.97e-04, Loss=6.27e-02 BER=2.37e-02 FER=3.17e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 56 Train Time 25.30045509338379s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.34it/s]Training epoch 57, Batch 1000/1000: LR=4.97e-04, Loss=6.22e-02 BER=2.35e-02 FER=3.15e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 57 Train Time 25.29869818687439s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 58, Batch 1000/1000: LR=4.97e-04, Loss=6.24e-02 BER=2.37e-02 FER=3.13e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 58 Train Time 25.29137635231018s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 59, Batch 1000/1000: LR=4.97e-04, Loss=6.14e-02 BER=2.33e-02 FER=3.10e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 59 Train Time 25.267045736312866s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 60, Batch 1000/1000: LR=4.97e-04, Loss=6.19e-02 BER=2.35e-02 FER=3.11e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 60 Train Time 25.309078693389893s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n",
      "Test EbN0=0, BER=1.52e-01 -ln(BER)=1.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Test EbN0=1, BER=1.11e-01 -ln(BER)=2.20e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Test EbN0=2, BER=8.04e-02 -ln(BER)=2.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Test EbN0=3, BER=3.82e-02 -ln(BER)=3.26e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Test EbN0=4, BER=1.72e-02 -ln(BER)=4.06e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Test EbN0=5, BER=5.46e-03 -ln(BER)=5.21e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Test EbN0=6, BER=1.20e-03 -ln(BER)=6.73e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "Test EbN0=7, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.56it/s]Training epoch 61, Batch 1000/1000: LR=4.97e-04, Loss=6.12e-02 BER=2.32e-02 FER=3.06e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.61it/s]\n",
      "Epoch 61 Train Time 25.245445013046265s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.42it/s]Training epoch 62, Batch 1000/1000: LR=4.97e-04, Loss=6.09e-02 BER=2.32e-02 FER=3.05e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 62 Train Time 25.305826902389526s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.58it/s]Training epoch 63, Batch 1000/1000: LR=4.97e-04, Loss=6.09e-02 BER=2.32e-02 FER=3.03e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 63 Train Time 25.29904341697693s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.50it/s]Training epoch 64, Batch 1000/1000: LR=4.96e-04, Loss=6.06e-02 BER=2.31e-02 FER=3.05e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 64 Train Time 25.33779239654541s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.50it/s]Training epoch 65, Batch 1000/1000: LR=4.96e-04, Loss=6.06e-02 BER=2.30e-02 FER=3.02e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.65it/s]\n",
      "Epoch 65 Train Time 25.218098878860474s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.40it/s]Training epoch 66, Batch 1000/1000: LR=4.96e-04, Loss=6.05e-02 BER=2.32e-02 FER=3.03e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.38it/s]\n",
      "Epoch 66 Train Time 25.396910429000854s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.50it/s]Training epoch 67, Batch 1000/1000: LR=4.96e-04, Loss=6.00e-02 BER=2.30e-02 FER=2.99e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 67 Train Time 25.306797742843628s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.34it/s]Training epoch 68, Batch 1000/1000: LR=4.96e-04, Loss=6.09e-02 BER=2.33e-02 FER=3.04e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.38it/s]\n",
      "Epoch 68 Train Time 25.397357940673828s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.61it/s]Training epoch 69, Batch 1000/1000: LR=4.96e-04, Loss=6.03e-02 BER=2.30e-02 FER=3.00e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 69 Train Time 25.300710439682007s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.66it/s]Training epoch 70, Batch 1000/1000: LR=4.96e-04, Loss=5.89e-02 BER=2.26e-02 FER=2.96e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 70 Train Time 25.329458713531494s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n",
      "Test EbN0=0, BER=1.49e-01 -ln(BER)=1.91e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Test EbN0=1, BER=1.16e-01 -ln(BER)=2.15e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Test EbN0=2, BER=7.67e-02 -ln(BER)=2.57e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Test EbN0=3, BER=3.77e-02 -ln(BER)=3.28e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Test EbN0=4, BER=1.65e-02 -ln(BER)=4.10e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Test EbN0=5, BER=3.83e-03 -ln(BER)=5.57e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "Test EbN0=6, BER=4.78e-04 -ln(BER)=7.65e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Test EbN0=7, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.53it/s]Training epoch 71, Batch 1000/1000: LR=4.95e-04, Loss=5.98e-02 BER=2.29e-02 FER=2.97e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 71 Train Time 25.33303165435791s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.38it/s]Training epoch 72, Batch 1000/1000: LR=4.95e-04, Loss=5.98e-02 BER=2.29e-02 FER=2.96e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 72 Train Time 25.33347201347351s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.51it/s]Training epoch 73, Batch 1000/1000: LR=4.95e-04, Loss=5.93e-02 BER=2.26e-02 FER=2.94e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 73 Train Time 25.262328386306763s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.49it/s]Training epoch 74, Batch 1000/1000: LR=4.95e-04, Loss=5.93e-02 BER=2.27e-02 FER=2.95e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 74 Train Time 25.284764051437378s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.68it/s]Training epoch 75, Batch 1000/1000: LR=4.95e-04, Loss=5.87e-02 BER=2.25e-02 FER=2.92e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 75 Train Time 25.262844562530518s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.75it/s]Training epoch 76, Batch 1000/1000: LR=4.95e-04, Loss=5.97e-02 BER=2.27e-02 FER=2.94e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 76 Train Time 25.30460548400879s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.15it/s]Training epoch 77, Batch 1000/1000: LR=4.94e-04, Loss=5.90e-02 BER=2.26e-02 FER=2.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 77 Train Time 25.35482120513916s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.40it/s]Training epoch 78, Batch 1000/1000: LR=4.94e-04, Loss=5.89e-02 BER=2.27e-02 FER=2.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.31it/s]\n",
      "Epoch 78 Train Time 25.441444873809814s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.27it/s]Training epoch 79, Batch 1000/1000: LR=4.94e-04, Loss=5.92e-02 BER=2.28e-02 FER=2.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 79 Train Time 25.261685371398926s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.65it/s]Training epoch 80, Batch 1000/1000: LR=4.94e-04, Loss=5.85e-02 BER=2.25e-02 FER=2.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 80 Train Time 25.311014652252197s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Test EbN0=0, BER=1.51e-01 -ln(BER)=1.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Test EbN0=1, BER=1.15e-01 -ln(BER)=2.16e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Test EbN0=2, BER=7.50e-02 -ln(BER)=2.59e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n",
      "Test EbN0=3, BER=3.85e-02 -ln(BER)=3.26e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Test EbN0=4, BER=1.20e-02 -ln(BER)=4.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Test EbN0=5, BER=3.47e-03 -ln(BER)=5.66e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
      "Test EbN0=6, BER=8.37e-04 -ln(BER)=7.09e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Test EbN0=7, BER=1.20e-04 -ln(BER)=9.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 81, Batch 1000/1000: LR=4.94e-04, Loss=5.86e-02 BER=2.26e-02 FER=2.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 81 Train Time 25.358263969421387s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.86it/s]Training epoch 82, Batch 1000/1000: LR=4.94e-04, Loss=5.89e-02 BER=2.27e-02 FER=2.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 82 Train Time 25.31217384338379s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.10it/s]Training epoch 83, Batch 1000/1000: LR=4.93e-04, Loss=5.78e-02 BER=2.23e-02 FER=2.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 83 Train Time 25.299195051193237s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 38.81it/s]Training epoch 84, Batch 1000/1000: LR=4.93e-04, Loss=5.84e-02 BER=2.25e-02 FER=2.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 84 Train Time 25.28243398666382s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.31it/s]Training epoch 85, Batch 1000/1000: LR=4.93e-04, Loss=5.82e-02 BER=2.25e-02 FER=2.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.41it/s]\n",
      "Epoch 85 Train Time 25.375900268554688s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.65it/s]Training epoch 86, Batch 1000/1000: LR=4.93e-04, Loss=5.80e-02 BER=2.23e-02 FER=2.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 86 Train Time 25.291779279708862s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.11it/s]Training epoch 87, Batch 1000/1000: LR=4.93e-04, Loss=5.83e-02 BER=2.25e-02 FER=2.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 87 Train Time 25.27164649963379s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.42it/s]Training epoch 88, Batch 1000/1000: LR=4.93e-04, Loss=5.79e-02 BER=2.23e-02 FER=2.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 88 Train Time 25.34777808189392s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 89, Batch 1000/1000: LR=4.92e-04, Loss=5.76e-02 BER=2.23e-02 FER=2.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 89 Train Time 25.298238277435303s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 37.59it/s]Training epoch 90, Batch 1000/1000: LR=4.92e-04, Loss=5.74e-02 BER=2.21e-02 FER=2.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 90 Train Time 25.25818181037903s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "Test EbN0=0, BER=1.54e-01 -ln(BER)=1.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Test EbN0=1, BER=1.14e-01 -ln(BER)=2.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Test EbN0=2, BER=6.94e-02 -ln(BER)=2.67e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.89it/s]\n",
      "Test EbN0=3, BER=4.15e-02 -ln(BER)=3.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Test EbN0=4, BER=1.45e-02 -ln(BER)=4.23e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Test EbN0=5, BER=2.99e-03 -ln(BER)=5.81e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n",
      "Test EbN0=6, BER=7.97e-04 -ln(BER)=7.13e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.54it/s]Training epoch 91, Batch 1000/1000: LR=4.92e-04, Loss=5.77e-02 BER=2.23e-02 FER=2.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 91 Train Time 25.319182634353638s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.47it/s]Training epoch 92, Batch 1000/1000: LR=4.92e-04, Loss=5.77e-02 BER=2.23e-02 FER=2.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 92 Train Time 25.31091547012329s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.62it/s]Training epoch 93, Batch 1000/1000: LR=4.92e-04, Loss=5.74e-02 BER=2.22e-02 FER=2.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 93 Train Time 25.32969903945923s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.07it/s]Training epoch 94, Batch 1000/1000: LR=4.91e-04, Loss=5.81e-02 BER=2.24e-02 FER=2.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 94 Train Time 25.34678864479065s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.30it/s]Training epoch 95, Batch 1000/1000: LR=4.91e-04, Loss=5.81e-02 BER=2.25e-02 FER=2.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 95 Train Time 25.319409132003784s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 96, Batch 1000/1000: LR=4.91e-04, Loss=5.80e-02 BER=2.25e-02 FER=2.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 96 Train Time 25.327289819717407s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.58it/s]Training epoch 97, Batch 1000/1000: LR=4.91e-04, Loss=5.77e-02 BER=2.23e-02 FER=2.85e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 97 Train Time 25.340357780456543s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.43it/s]Training epoch 98, Batch 1000/1000: LR=4.91e-04, Loss=5.77e-02 BER=2.23e-02 FER=2.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 98 Train Time 25.30194640159607s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.39it/s]Training epoch 99, Batch 1000/1000: LR=4.90e-04, Loss=5.74e-02 BER=2.22e-02 FER=2.83e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 99 Train Time 25.29992938041687s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.75it/s]Training epoch 100, Batch 1000/1000: LR=4.90e-04, Loss=5.78e-02 BER=2.23e-02 FER=2.82e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 100 Train Time 25.27250337600708s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Test EbN0=0, BER=1.49e-01 -ln(BER)=1.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Test EbN0=1, BER=1.11e-01 -ln(BER)=2.20e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Test EbN0=2, BER=6.94e-02 -ln(BER)=2.67e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Test EbN0=3, BER=3.66e-02 -ln(BER)=3.31e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Test EbN0=4, BER=1.29e-02 -ln(BER)=4.35e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Test EbN0=5, BER=3.27e-03 -ln(BER)=5.72e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
      "Test EbN0=6, BER=5.18e-04 -ln(BER)=7.57e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.92it/s]Training epoch 101, Batch 1000/1000: LR=4.90e-04, Loss=5.72e-02 BER=2.22e-02 FER=2.81e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 101 Train Time 25.313548803329468s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.37it/s]Training epoch 102, Batch 1000/1000: LR=4.90e-04, Loss=5.71e-02 BER=2.21e-02 FER=2.77e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 102 Train Time 25.323171377182007s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.58it/s]Training epoch 103, Batch 1000/1000: LR=4.89e-04, Loss=5.68e-02 BER=2.18e-02 FER=2.79e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 103 Train Time 25.30788540840149s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.40it/s]Training epoch 104, Batch 1000/1000: LR=4.89e-04, Loss=5.71e-02 BER=2.21e-02 FER=2.81e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 104 Train Time 25.322542428970337s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.49it/s]Training epoch 105, Batch 1000/1000: LR=4.89e-04, Loss=5.74e-02 BER=2.23e-02 FER=2.81e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 105 Train Time 25.34945249557495s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.26it/s]Training epoch 106, Batch 1000/1000: LR=4.89e-04, Loss=5.68e-02 BER=2.20e-02 FER=2.80e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 106 Train Time 25.26675772666931s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.62it/s]Training epoch 107, Batch 1000/1000: LR=4.88e-04, Loss=5.69e-02 BER=2.21e-02 FER=2.79e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 107 Train Time 25.296632766723633s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 108, Batch 1000/1000: LR=4.88e-04, Loss=5.67e-02 BER=2.19e-02 FER=2.79e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 108 Train Time 25.333041667938232s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.47it/s]Training epoch 109, Batch 1000/1000: LR=4.88e-04, Loss=5.71e-02 BER=2.22e-02 FER=2.80e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.60it/s]\n",
      "Epoch 109 Train Time 25.255696296691895s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.40it/s]Training epoch 110, Batch 1000/1000: LR=4.88e-04, Loss=5.65e-02 BER=2.19e-02 FER=2.80e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 110 Train Time 25.32536005973816s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Test EbN0=0, BER=1.51e-01 -ln(BER)=1.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Test EbN0=1, BER=1.14e-01 -ln(BER)=2.17e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n",
      "Test EbN0=2, BER=6.78e-02 -ln(BER)=2.69e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
      "Test EbN0=3, BER=3.69e-02 -ln(BER)=3.30e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "Test EbN0=4, BER=1.46e-02 -ln(BER)=4.23e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Test EbN0=5, BER=4.58e-03 -ln(BER)=5.39e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "Test EbN0=6, BER=5.98e-04 -ln(BER)=7.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Test EbN0=7, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.17it/s]\n",
      "Test EbN0=8, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.55it/s]Training epoch 111, Batch 1000/1000: LR=4.88e-04, Loss=5.64e-02 BER=2.18e-02 FER=2.78e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 111 Train Time 25.332301139831543s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 112, Batch 1000/1000: LR=4.87e-04, Loss=5.71e-02 BER=2.21e-02 FER=2.79e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 112 Train Time 25.33147168159485s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.61it/s]Training epoch 113, Batch 1000/1000: LR=4.87e-04, Loss=5.68e-02 BER=2.18e-02 FER=2.77e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 113 Train Time 25.292699813842773s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.29it/s]Training epoch 114, Batch 1000/1000: LR=4.87e-04, Loss=5.66e-02 BER=2.19e-02 FER=2.78e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 114 Train Time 25.3131685256958s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 115, Batch 1000/1000: LR=4.87e-04, Loss=5.64e-02 BER=2.19e-02 FER=2.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 115 Train Time 25.30730152130127s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 116, Batch 1000/1000: LR=4.86e-04, Loss=5.67e-02 BER=2.19e-02 FER=2.79e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 116 Train Time 25.321972370147705s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.49it/s]Training epoch 117, Batch 1000/1000: LR=4.86e-04, Loss=5.66e-02 BER=2.18e-02 FER=2.79e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 117 Train Time 25.322901725769043s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 118, Batch 1000/1000: LR=4.86e-04, Loss=5.65e-02 BER=2.19e-02 FER=2.77e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 118 Train Time 25.294286489486694s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.49it/s]Training epoch 119, Batch 1000/1000: LR=4.85e-04, Loss=5.63e-02 BER=2.18e-02 FER=2.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 119 Train Time 25.291298627853394s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.56it/s]Training epoch 120, Batch 1000/1000: LR=4.85e-04, Loss=5.68e-02 BER=2.20e-02 FER=2.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 120 Train Time 25.339059352874756s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Test EbN0=0, BER=1.49e-01 -ln(BER)=1.91e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=1, BER=1.17e-01 -ln(BER)=2.15e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Test EbN0=2, BER=7.28e-02 -ln(BER)=2.62e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Test EbN0=3, BER=3.54e-02 -ln(BER)=3.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Test EbN0=4, BER=1.54e-02 -ln(BER)=4.17e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Test EbN0=5, BER=3.63e-03 -ln(BER)=5.62e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Test EbN0=6, BER=8.77e-04 -ln(BER)=7.04e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n",
      "Test EbN0=7, BER=1.20e-04 -ln(BER)=9.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.31it/s]Training epoch 121, Batch 1000/1000: LR=4.85e-04, Loss=5.63e-02 BER=2.18e-02 FER=2.75e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 121 Train Time 25.311463594436646s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.62it/s]Training epoch 122, Batch 1000/1000: LR=4.85e-04, Loss=5.62e-02 BER=2.18e-02 FER=2.78e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 122 Train Time 25.2609646320343s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.55it/s]Training epoch 123, Batch 1000/1000: LR=4.84e-04, Loss=5.62e-02 BER=2.18e-02 FER=2.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 123 Train Time 25.271862268447876s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.16it/s]Training epoch 124, Batch 1000/1000: LR=4.84e-04, Loss=5.65e-02 BER=2.19e-02 FER=2.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 124 Train Time 25.33572006225586s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.21it/s]Training epoch 125, Batch 1000/1000: LR=4.84e-04, Loss=5.65e-02 BER=2.20e-02 FER=2.75e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 125 Train Time 25.353262662887573s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.79it/s]Training epoch 126, Batch 1000/1000: LR=4.84e-04, Loss=5.65e-02 BER=2.20e-02 FER=2.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 126 Train Time 25.285353422164917s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.09it/s]Training epoch 127, Batch 1000/1000: LR=4.83e-04, Loss=5.64e-02 BER=2.18e-02 FER=2.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 127 Train Time 25.266054153442383s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 128, Batch 1000/1000: LR=4.83e-04, Loss=5.65e-02 BER=2.19e-02 FER=2.75e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 128 Train Time 25.352055072784424s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.32it/s]Training epoch 129, Batch 1000/1000: LR=4.83e-04, Loss=5.61e-02 BER=2.18e-02 FER=2.75e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 129 Train Time 25.29005718231201s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 130, Batch 1000/1000: LR=4.82e-04, Loss=5.61e-02 BER=2.18e-02 FER=2.74e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 130 Train Time 25.33847999572754s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n",
      "Test EbN0=0, BER=1.51e-01 -ln(BER)=1.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Test EbN0=1, BER=1.10e-01 -ln(BER)=2.21e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Test EbN0=2, BER=6.95e-02 -ln(BER)=2.67e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Test EbN0=3, BER=4.22e-02 -ln(BER)=3.17e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Test EbN0=4, BER=1.45e-02 -ln(BER)=4.24e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Test EbN0=5, BER=3.99e-03 -ln(BER)=5.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Test EbN0=6, BER=9.57e-04 -ln(BER)=6.95e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.51it/s]Training epoch 131, Batch 1000/1000: LR=4.82e-04, Loss=5.60e-02 BER=2.17e-02 FER=2.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 131 Train Time 25.263556957244873s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.69it/s]Training epoch 132, Batch 1000/1000: LR=4.82e-04, Loss=5.62e-02 BER=2.18e-02 FER=2.74e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.60it/s]\n",
      "Epoch 132 Train Time 25.250123023986816s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.41it/s]Training epoch 133, Batch 1000/1000: LR=4.82e-04, Loss=5.58e-02 BER=2.16e-02 FER=2.73e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 133 Train Time 25.31765604019165s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.64it/s]Training epoch 134, Batch 1000/1000: LR=4.81e-04, Loss=5.62e-02 BER=2.18e-02 FER=2.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 134 Train Time 25.28512954711914s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.40it/s]Training epoch 135, Batch 1000/1000: LR=4.81e-04, Loss=5.61e-02 BER=2.19e-02 FER=2.74e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 135 Train Time 25.32136368751526s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.41it/s]Training epoch 136, Batch 1000/1000: LR=4.81e-04, Loss=5.58e-02 BER=2.16e-02 FER=2.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 136 Train Time 25.356661319732666s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 137, Batch 1000/1000: LR=4.80e-04, Loss=5.56e-02 BER=2.16e-02 FER=2.71e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 137 Train Time 25.35755181312561s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 138, Batch 1000/1000: LR=4.80e-04, Loss=5.61e-02 BER=2.18e-02 FER=2.73e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 138 Train Time 25.324029684066772s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.45it/s]Training epoch 139, Batch 1000/1000: LR=4.80e-04, Loss=5.63e-02 BER=2.19e-02 FER=2.74e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 139 Train Time 25.329424381256104s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 140, Batch 1000/1000: LR=4.79e-04, Loss=5.53e-02 BER=2.15e-02 FER=2.71e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 140 Train Time 25.349709510803223s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "Test EbN0=0, BER=1.49e-01 -ln(BER)=1.91e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n",
      "Test EbN0=1, BER=1.14e-01 -ln(BER)=2.17e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Test EbN0=2, BER=7.28e-02 -ln(BER)=2.62e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=3, BER=3.81e-02 -ln(BER)=3.27e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Test EbN0=4, BER=1.47e-02 -ln(BER)=4.22e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "Test EbN0=5, BER=3.15e-03 -ln(BER)=5.76e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "Test EbN0=6, BER=5.58e-04 -ln(BER)=7.49e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.22it/s]Training epoch 141, Batch 1000/1000: LR=4.79e-04, Loss=5.54e-02 BER=2.16e-02 FER=2.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 141 Train Time 25.363527059555054s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 142, Batch 1000/1000: LR=4.79e-04, Loss=5.60e-02 BER=2.18e-02 FER=2.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.63it/s]\n",
      "Epoch 142 Train Time 25.235687017440796s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.11it/s]Training epoch 143, Batch 1000/1000: LR=4.78e-04, Loss=5.58e-02 BER=2.16e-02 FER=2.71e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 143 Train Time 25.32684850692749s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.55it/s]Training epoch 144, Batch 1000/1000: LR=4.78e-04, Loss=5.56e-02 BER=2.17e-02 FER=2.71e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 144 Train Time 25.30106496810913s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.57it/s]Training epoch 145, Batch 1000/1000: LR=4.78e-04, Loss=5.57e-02 BER=2.17e-02 FER=2.70e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 145 Train Time 25.300988912582397s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.17it/s]Training epoch 146, Batch 1000/1000: LR=4.78e-04, Loss=5.57e-02 BER=2.16e-02 FER=2.71e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 146 Train Time 25.332088708877563s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.01it/s]Training epoch 147, Batch 1000/1000: LR=4.77e-04, Loss=5.58e-02 BER=2.17e-02 FER=2.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 147 Train Time 25.27811074256897s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.64it/s]Training epoch 148, Batch 1000/1000: LR=4.77e-04, Loss=5.61e-02 BER=2.17e-02 FER=2.70e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 148 Train Time 25.320992946624756s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.48it/s]Training epoch 149, Batch 1000/1000: LR=4.77e-04, Loss=5.57e-02 BER=2.17e-02 FER=2.69e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 149 Train Time 25.290249586105347s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 150, Batch 1000/1000: LR=4.76e-04, Loss=5.54e-02 BER=2.15e-02 FER=2.69e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 150 Train Time 25.27352499961853s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "Test EbN0=0, BER=1.50e-01 -ln(BER)=1.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Test EbN0=1, BER=1.10e-01 -ln(BER)=2.21e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Test EbN0=2, BER=7.01e-02 -ln(BER)=2.66e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Test EbN0=3, BER=3.88e-02 -ln(BER)=3.25e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Test EbN0=4, BER=1.75e-02 -ln(BER)=4.05e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Test EbN0=5, BER=3.27e-03 -ln(BER)=5.72e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "Test EbN0=6, BER=2.79e-04 -ln(BER)=8.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Test EbN0=7, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.49it/s]Training epoch 151, Batch 1000/1000: LR=4.76e-04, Loss=5.51e-02 BER=2.15e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 151 Train Time 25.265264987945557s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.30it/s]Training epoch 152, Batch 1000/1000: LR=4.76e-04, Loss=5.56e-02 BER=2.16e-02 FER=2.70e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 152 Train Time 25.323646783828735s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.15it/s]Training epoch 153, Batch 1000/1000: LR=4.75e-04, Loss=5.61e-02 BER=2.20e-02 FER=2.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.42it/s]\n",
      "Epoch 153 Train Time 25.370698928833008s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.31it/s]Training epoch 154, Batch 1000/1000: LR=4.75e-04, Loss=5.54e-02 BER=2.16e-02 FER=2.69e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 154 Train Time 25.330243349075317s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.23it/s]Training epoch 155, Batch 1000/1000: LR=4.75e-04, Loss=5.55e-02 BER=2.17e-02 FER=2.70e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 155 Train Time 25.3542697429657s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.16it/s]Training epoch 156, Batch 1000/1000: LR=4.74e-04, Loss=5.58e-02 BER=2.17e-02 FER=2.69e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.60it/s]\n",
      "Epoch 156 Train Time 25.252906799316406s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.53it/s]Training epoch 157, Batch 1000/1000: LR=4.74e-04, Loss=5.53e-02 BER=2.16e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 157 Train Time 25.28153681755066s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.48it/s]Training epoch 158, Batch 1000/1000: LR=4.73e-04, Loss=5.52e-02 BER=2.14e-02 FER=2.66e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 158 Train Time 25.309351682662964s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 159, Batch 1000/1000: LR=4.73e-04, Loss=5.51e-02 BER=2.13e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 159 Train Time 25.307788133621216s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.00it/s]Training epoch 160, Batch 1000/1000: LR=4.73e-04, Loss=5.51e-02 BER=2.15e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 160 Train Time 25.31747555732727s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n",
      "Test EbN0=0, BER=1.51e-01 -ln(BER)=1.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Test EbN0=1, BER=1.16e-01 -ln(BER)=2.16e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Test EbN0=2, BER=7.34e-02 -ln(BER)=2.61e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Test EbN0=3, BER=3.95e-02 -ln(BER)=3.23e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Test EbN0=4, BER=1.14e-02 -ln(BER)=4.47e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "Test EbN0=5, BER=3.43e-03 -ln(BER)=5.68e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Test EbN0=6, BER=4.78e-04 -ln(BER)=7.65e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.42it/s]Training epoch 161, Batch 1000/1000: LR=4.72e-04, Loss=5.54e-02 BER=2.17e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 161 Train Time 25.328284740447998s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.42it/s]Training epoch 162, Batch 1000/1000: LR=4.72e-04, Loss=5.52e-02 BER=2.14e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 162 Train Time 25.278409719467163s\n",
      "\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.49it/s]Training epoch 163, Batch 1000/1000: LR=4.72e-04, Loss=5.45e-02 BER=2.12e-02 FER=2.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.35it/s]\n",
      "Epoch 163 Train Time 25.413445472717285s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.42it/s]Training epoch 164, Batch 1000/1000: LR=4.71e-04, Loss=5.50e-02 BER=2.15e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 164 Train Time 25.306519269943237s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.39it/s]Training epoch 165, Batch 1000/1000: LR=4.71e-04, Loss=5.57e-02 BER=2.16e-02 FER=2.69e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 165 Train Time 25.309213638305664s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.37it/s]Training epoch 166, Batch 1000/1000: LR=4.71e-04, Loss=5.45e-02 BER=2.12e-02 FER=2.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 166 Train Time 25.25833821296692s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.57it/s]Training epoch 167, Batch 1000/1000: LR=4.70e-04, Loss=5.47e-02 BER=2.14e-02 FER=2.66e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 167 Train Time 25.307438373565674s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.42it/s]Training epoch 168, Batch 1000/1000: LR=4.70e-04, Loss=5.53e-02 BER=2.16e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 168 Train Time 25.320160150527954s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.03it/s]Training epoch 169, Batch 1000/1000: LR=4.69e-04, Loss=5.55e-02 BER=2.16e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.41it/s]\n",
      "Epoch 169 Train Time 25.37485980987549s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.53it/s]Training epoch 170, Batch 1000/1000: LR=4.69e-04, Loss=5.53e-02 BER=2.15e-02 FER=2.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 170 Train Time 25.31026339530945s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.29it/s]\n",
      "Test EbN0=0, BER=1.46e-01 -ln(BER)=1.92e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]\n",
      "Test EbN0=1, BER=1.19e-01 -ln(BER)=2.13e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Test EbN0=2, BER=7.86e-02 -ln(BER)=2.54e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Test EbN0=3, BER=3.82e-02 -ln(BER)=3.26e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Test EbN0=4, BER=1.40e-02 -ln(BER)=4.27e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Test EbN0=5, BER=3.79e-03 -ln(BER)=5.58e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Test EbN0=6, BER=1.20e-04 -ln(BER)=9.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Test EbN0=7, BER=1.59e-04 -ln(BER)=8.74e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.62it/s]Training epoch 171, Batch 1000/1000: LR=4.69e-04, Loss=5.50e-02 BER=2.15e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 171 Train Time 25.269503831863403s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.98it/s]Training epoch 172, Batch 1000/1000: LR=4.68e-04, Loss=5.44e-02 BER=2.12e-02 FER=2.66e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 172 Train Time 25.282320976257324s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.66it/s]Training epoch 173, Batch 1000/1000: LR=4.68e-04, Loss=5.47e-02 BER=2.14e-02 FER=2.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 173 Train Time 25.278667211532593s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.42it/s]Training epoch 174, Batch 1000/1000: LR=4.68e-04, Loss=5.48e-02 BER=2.14e-02 FER=2.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 174 Train Time 25.337531328201294s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.58it/s]Training epoch 175, Batch 1000/1000: LR=4.67e-04, Loss=5.45e-02 BER=2.14e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 175 Train Time 25.350188970565796s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.53it/s]Training epoch 176, Batch 1000/1000: LR=4.67e-04, Loss=5.51e-02 BER=2.14e-02 FER=2.66e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 176 Train Time 25.324793815612793s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.44it/s]Training epoch 177, Batch 1000/1000: LR=4.66e-04, Loss=5.50e-02 BER=2.15e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 177 Train Time 25.37869930267334s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.52it/s]Training epoch 178, Batch 1000/1000: LR=4.66e-04, Loss=5.42e-02 BER=2.12e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 178 Train Time 25.28709363937378s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.56it/s]Training epoch 179, Batch 1000/1000: LR=4.66e-04, Loss=5.48e-02 BER=2.15e-02 FER=2.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 179 Train Time 25.32114601135254s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.46it/s]Training epoch 180, Batch 1000/1000: LR=4.65e-04, Loss=5.44e-02 BER=2.12e-02 FER=2.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 180 Train Time 25.307453870773315s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=0, BER=1.48e-01 -ln(BER)=1.91e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
      "Test EbN0=1, BER=1.13e-01 -ln(BER)=2.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.46it/s]\n",
      "Test EbN0=2, BER=6.86e-02 -ln(BER)=2.68e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Test EbN0=3, BER=3.64e-02 -ln(BER)=3.31e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Test EbN0=4, BER=1.43e-02 -ln(BER)=4.25e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=5, BER=4.42e-03 -ln(BER)=5.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Test EbN0=6, BER=7.17e-04 -ln(BER)=7.24e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.13it/s]Training epoch 181, Batch 1000/1000: LR=4.65e-04, Loss=5.45e-02 BER=2.12e-02 FER=2.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 181 Train Time 25.288939237594604s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.30it/s]Training epoch 182, Batch 1000/1000: LR=4.64e-04, Loss=5.48e-02 BER=2.14e-02 FER=2.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 182 Train Time 25.312568187713623s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.39it/s]Training epoch 183, Batch 1000/1000: LR=4.64e-04, Loss=5.52e-02 BER=2.15e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 183 Train Time 25.28490424156189s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 184, Batch 1000/1000: LR=4.64e-04, Loss=5.50e-02 BER=2.16e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 184 Train Time 25.32237482070923s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 185, Batch 1000/1000: LR=4.63e-04, Loss=5.43e-02 BER=2.12e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 185 Train Time 25.33894395828247s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.81it/s]Training epoch 186, Batch 1000/1000: LR=4.63e-04, Loss=5.53e-02 BER=2.16e-02 FER=2.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 186 Train Time 25.317591190338135s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.42it/s]Training epoch 187, Batch 1000/1000: LR=4.62e-04, Loss=5.46e-02 BER=2.13e-02 FER=2.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 187 Train Time 25.256902933120728s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.61it/s]Training epoch 188, Batch 1000/1000: LR=4.62e-04, Loss=5.48e-02 BER=2.13e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.69it/s]\n",
      "Epoch 188 Train Time 25.193377017974854s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.07it/s]Training epoch 189, Batch 1000/1000: LR=4.62e-04, Loss=5.41e-02 BER=2.11e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 189 Train Time 25.293699502944946s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.62it/s]Training epoch 190, Batch 1000/1000: LR=4.61e-04, Loss=5.46e-02 BER=2.14e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.60it/s]\n",
      "Epoch 190 Train Time 25.253123998641968s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Test EbN0=0, BER=1.48e-01 -ln(BER)=1.91e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Test EbN0=1, BER=1.10e-01 -ln(BER)=2.20e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.12it/s]\n",
      "Test EbN0=2, BER=7.29e-02 -ln(BER)=2.62e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "Test EbN0=3, BER=3.75e-02 -ln(BER)=3.28e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "Test EbN0=4, BER=1.61e-02 -ln(BER)=4.13e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n",
      "Test EbN0=5, BER=2.87e-03 -ln(BER)=5.85e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n",
      "Test EbN0=6, BER=5.18e-04 -ln(BER)=7.57e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.28it/s]Training epoch 191, Batch 1000/1000: LR=4.61e-04, Loss=5.48e-02 BER=2.14e-02 FER=2.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 191 Train Time 25.35497212409973s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.07it/s]Training epoch 192, Batch 1000/1000: LR=4.60e-04, Loss=5.38e-02 BER=2.11e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 192 Train Time 25.293530464172363s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.48it/s]Training epoch 193, Batch 1000/1000: LR=4.60e-04, Loss=5.46e-02 BER=2.13e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 193 Train Time 25.29072117805481s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.26it/s]Training epoch 194, Batch 1000/1000: LR=4.59e-04, Loss=5.44e-02 BER=2.14e-02 FER=2.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.39it/s]\n",
      "Epoch 194 Train Time 25.384804010391235s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.56it/s]Training epoch 195, Batch 1000/1000: LR=4.59e-04, Loss=5.44e-02 BER=2.13e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 195 Train Time 25.276143312454224s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.59it/s]Training epoch 196, Batch 1000/1000: LR=4.59e-04, Loss=5.46e-02 BER=2.14e-02 FER=2.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 196 Train Time 25.308982133865356s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.50it/s]Training epoch 197, Batch 1000/1000: LR=4.58e-04, Loss=5.48e-02 BER=2.14e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 197 Train Time 25.294408559799194s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.45it/s]Training epoch 198, Batch 1000/1000: LR=4.58e-04, Loss=5.41e-02 BER=2.11e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 198 Train Time 25.35331630706787s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.49it/s]Training epoch 199, Batch 1000/1000: LR=4.57e-04, Loss=5.40e-02 BER=2.11e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 199 Train Time 25.340593099594116s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.45it/s]Training epoch 200, Batch 1000/1000: LR=4.57e-04, Loss=5.47e-02 BER=2.13e-02 FER=2.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 200 Train Time 25.30006194114685s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n",
      "Test EbN0=0, BER=1.54e-01 -ln(BER)=1.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Test EbN0=1, BER=1.11e-01 -ln(BER)=2.20e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]\n",
      "Test EbN0=2, BER=7.07e-02 -ln(BER)=2.65e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Test EbN0=3, BER=3.86e-02 -ln(BER)=3.25e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]\n",
      "Test EbN0=4, BER=1.14e-02 -ln(BER)=4.47e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=5, BER=3.63e-03 -ln(BER)=5.62e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Test EbN0=6, BER=3.59e-04 -ln(BER)=7.93e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.51it/s]Training epoch 201, Batch 1000/1000: LR=4.56e-04, Loss=5.48e-02 BER=2.15e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 201 Train Time 25.28764295578003s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.46it/s]Training epoch 202, Batch 1000/1000: LR=4.56e-04, Loss=5.43e-02 BER=2.12e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 202 Train Time 25.302968978881836s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.33it/s]Training epoch 203, Batch 1000/1000: LR=4.55e-04, Loss=5.42e-02 BER=2.12e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 203 Train Time 25.32712984085083s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 204, Batch 1000/1000: LR=4.55e-04, Loss=5.43e-02 BER=2.12e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 204 Train Time 25.303662538528442s\n",
      "\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.55it/s]Training epoch 205, Batch 1000/1000: LR=4.55e-04, Loss=5.40e-02 BER=2.11e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 205 Train Time 25.361706256866455s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.53it/s]Training epoch 206, Batch 1000/1000: LR=4.54e-04, Loss=5.38e-02 BER=2.11e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 206 Train Time 25.30036687850952s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 207, Batch 1000/1000: LR=4.54e-04, Loss=5.45e-02 BER=2.14e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 207 Train Time 25.26566791534424s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.55it/s]Training epoch 208, Batch 1000/1000: LR=4.53e-04, Loss=5.41e-02 BER=2.12e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 208 Train Time 25.290891408920288s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.40it/s]Training epoch 209, Batch 1000/1000: LR=4.53e-04, Loss=5.43e-02 BER=2.13e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 209 Train Time 25.31733274459839s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.55it/s]Training epoch 210, Batch 1000/1000: LR=4.52e-04, Loss=5.44e-02 BER=2.13e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 210 Train Time 25.303704261779785s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.32it/s]\n",
      "Test EbN0=0, BER=1.53e-01 -ln(BER)=1.88e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Test EbN0=1, BER=1.11e-01 -ln(BER)=2.19e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Test EbN0=2, BER=6.14e-02 -ln(BER)=2.79e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=3, BER=3.62e-02 -ln(BER)=3.32e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\n",
      "Test EbN0=4, BER=1.24e-02 -ln(BER)=4.39e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Test EbN0=5, BER=3.51e-03 -ln(BER)=5.65e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Test EbN0=6, BER=3.99e-04 -ln(BER)=7.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.14it/s]Training epoch 211, Batch 1000/1000: LR=4.52e-04, Loss=5.45e-02 BER=2.14e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 211 Train Time 25.328060388565063s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.48it/s]Training epoch 212, Batch 1000/1000: LR=4.51e-04, Loss=5.39e-02 BER=2.11e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 212 Train Time 25.287424564361572s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.26it/s]Training epoch 213, Batch 1000/1000: LR=4.51e-04, Loss=5.40e-02 BER=2.11e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 213 Train Time 25.332263946533203s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.49it/s]Training epoch 214, Batch 1000/1000: LR=4.50e-04, Loss=5.39e-02 BER=2.11e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 214 Train Time 25.34870958328247s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.20it/s]Training epoch 215, Batch 1000/1000: LR=4.50e-04, Loss=5.47e-02 BER=2.14e-02 FER=2.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 215 Train Time 25.31593894958496s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.31it/s]Training epoch 216, Batch 1000/1000: LR=4.49e-04, Loss=5.38e-02 BER=2.11e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 216 Train Time 25.363802433013916s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.43it/s]Training epoch 217, Batch 1000/1000: LR=4.49e-04, Loss=5.45e-02 BER=2.13e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 217 Train Time 25.30205535888672s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.40it/s]Training epoch 218, Batch 1000/1000: LR=4.48e-04, Loss=5.36e-02 BER=2.09e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.61it/s]\n",
      "Epoch 218 Train Time 25.244807958602905s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.42it/s]Training epoch 219, Batch 1000/1000: LR=4.48e-04, Loss=5.40e-02 BER=2.12e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 219 Train Time 25.33884334564209s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.30it/s]Training epoch 220, Batch 1000/1000: LR=4.48e-04, Loss=5.40e-02 BER=2.11e-02 FER=2.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 220 Train Time 25.311288833618164s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
      "Test EbN0=0, BER=1.50e-01 -ln(BER)=1.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Test EbN0=1, BER=1.16e-01 -ln(BER)=2.16e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "Test EbN0=2, BER=7.37e-02 -ln(BER)=2.61e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Test EbN0=3, BER=3.92e-02 -ln(BER)=3.24e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.85it/s]\n",
      "Test EbN0=4, BER=1.51e-02 -ln(BER)=4.20e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Test EbN0=5, BER=4.15e-03 -ln(BER)=5.49e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]\n",
      "Test EbN0=6, BER=4.38e-04 -ln(BER)=7.73e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.42it/s]Training epoch 221, Batch 1000/1000: LR=4.47e-04, Loss=5.40e-02 BER=2.11e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 221 Train Time 25.345496654510498s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.23it/s]Training epoch 222, Batch 1000/1000: LR=4.47e-04, Loss=5.38e-02 BER=2.11e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 222 Train Time 25.34151339530945s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.41it/s]Training epoch 223, Batch 1000/1000: LR=4.46e-04, Loss=5.44e-02 BER=2.13e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 223 Train Time 25.348931312561035s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.53it/s]Training epoch 224, Batch 1000/1000: LR=4.46e-04, Loss=5.41e-02 BER=2.11e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 224 Train Time 25.31497859954834s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.41it/s]Training epoch 225, Batch 1000/1000: LR=4.45e-04, Loss=5.46e-02 BER=2.13e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 225 Train Time 25.317694187164307s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.42it/s]Training epoch 226, Batch 1000/1000: LR=4.45e-04, Loss=5.39e-02 BER=2.10e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 226 Train Time 25.382308959960938s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.30it/s]Training epoch 227, Batch 1000/1000: LR=4.44e-04, Loss=5.43e-02 BER=2.13e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 227 Train Time 25.308698415756226s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.54it/s]Training epoch 228, Batch 1000/1000: LR=4.44e-04, Loss=5.36e-02 BER=2.10e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 228 Train Time 25.303730964660645s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.10it/s]Training epoch 229, Batch 1000/1000: LR=4.43e-04, Loss=5.37e-02 BER=2.11e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 229 Train Time 25.338541984558105s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.37it/s]Training epoch 230, Batch 1000/1000: LR=4.43e-04, Loss=5.42e-02 BER=2.14e-02 FER=2.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 230 Train Time 25.323768854141235s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n",
      "Test EbN0=0, BER=1.46e-01 -ln(BER)=1.92e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Test EbN0=1, BER=1.13e-01 -ln(BER)=2.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Test EbN0=2, BER=6.91e-02 -ln(BER)=2.67e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "Test EbN0=3, BER=3.67e-02 -ln(BER)=3.31e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n",
      "Test EbN0=4, BER=1.26e-02 -ln(BER)=4.37e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "Test EbN0=5, BER=3.99e-03 -ln(BER)=5.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Test EbN0=6, BER=4.78e-04 -ln(BER)=7.65e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.18it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.55it/s]Training epoch 231, Batch 1000/1000: LR=4.42e-04, Loss=5.38e-02 BER=2.11e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 231 Train Time 25.275036811828613s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.55it/s]Training epoch 232, Batch 1000/1000: LR=4.42e-04, Loss=5.36e-02 BER=2.09e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 232 Train Time 25.305458545684814s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.42it/s]Training epoch 233, Batch 1000/1000: LR=4.41e-04, Loss=5.34e-02 BER=2.09e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 233 Train Time 25.342741012573242s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.48it/s]Training epoch 234, Batch 1000/1000: LR=4.41e-04, Loss=5.37e-02 BER=2.11e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 234 Train Time 25.292557954788208s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 235, Batch 1000/1000: LR=4.40e-04, Loss=5.40e-02 BER=2.12e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 235 Train Time 25.27785849571228s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.40it/s]Training epoch 236, Batch 1000/1000: LR=4.40e-04, Loss=5.37e-02 BER=2.11e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 236 Train Time 25.302669286727905s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 237, Batch 1000/1000: LR=4.39e-04, Loss=5.36e-02 BER=2.11e-02 FER=2.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 237 Train Time 25.314265489578247s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.43it/s]Training epoch 238, Batch 1000/1000: LR=4.39e-04, Loss=5.38e-02 BER=2.10e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 238 Train Time 25.301110982894897s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.94it/s]Training epoch 239, Batch 1000/1000: LR=4.38e-04, Loss=5.35e-02 BER=2.09e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 239 Train Time 25.332072973251343s\n",
      "\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.09it/s]Training epoch 240, Batch 1000/1000: LR=4.38e-04, Loss=5.39e-02 BER=2.12e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 240 Train Time 25.308581590652466s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n",
      "Test EbN0=0, BER=1.52e-01 -ln(BER)=1.88e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Test EbN0=1, BER=1.13e-01 -ln(BER)=2.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.41it/s]\n",
      "Test EbN0=2, BER=7.10e-02 -ln(BER)=2.64e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
      "Test EbN0=3, BER=3.77e-02 -ln(BER)=3.28e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.85it/s]\n",
      "Test EbN0=4, BER=1.26e-02 -ln(BER)=4.37e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Test EbN0=5, BER=3.99e-03 -ln(BER)=5.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "Test EbN0=6, BER=1.59e-04 -ln(BER)=8.74e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=7, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 241, Batch 1000/1000: LR=4.37e-04, Loss=5.35e-02 BER=2.09e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 241 Train Time 25.355332851409912s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.00it/s]Training epoch 242, Batch 1000/1000: LR=4.36e-04, Loss=5.34e-02 BER=2.10e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 242 Train Time 25.29629397392273s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.64it/s]Training epoch 243, Batch 1000/1000: LR=4.36e-04, Loss=5.35e-02 BER=2.10e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 243 Train Time 25.335527896881104s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.36it/s]Training epoch 244, Batch 1000/1000: LR=4.35e-04, Loss=5.35e-02 BER=2.10e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 244 Train Time 25.301386833190918s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.89it/s]Training epoch 245, Batch 1000/1000: LR=4.35e-04, Loss=5.43e-02 BER=2.12e-02 FER=2.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.41it/s]\n",
      "Epoch 245 Train Time 25.372241973876953s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 246, Batch 1000/1000: LR=4.34e-04, Loss=5.33e-02 BER=2.08e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 246 Train Time 25.36096978187561s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.26it/s]Training epoch 247, Batch 1000/1000: LR=4.34e-04, Loss=5.32e-02 BER=2.10e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.42it/s]\n",
      "Epoch 247 Train Time 25.367781162261963s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.53it/s]Training epoch 248, Batch 1000/1000: LR=4.33e-04, Loss=5.36e-02 BER=2.11e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 248 Train Time 25.342819452285767s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.05it/s]Training epoch 249, Batch 1000/1000: LR=4.33e-04, Loss=5.37e-02 BER=2.12e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 249 Train Time 25.354713201522827s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.43it/s]Training epoch 250, Batch 1000/1000: LR=4.32e-04, Loss=5.34e-02 BER=2.11e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 250 Train Time 25.351208925247192s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Test EbN0=0, BER=1.53e-01 -ln(BER)=1.88e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Test EbN0=1, BER=1.09e-01 -ln(BER)=2.22e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Test EbN0=2, BER=7.35e-02 -ln(BER)=2.61e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n",
      "Test EbN0=3, BER=3.37e-02 -ln(BER)=3.39e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "Test EbN0=4, BER=1.63e-02 -ln(BER)=4.12e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "Test EbN0=5, BER=2.15e-03 -ln(BER)=6.14e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "Test EbN0=6, BER=1.04e-03 -ln(BER)=6.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.35it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.48it/s]Training epoch 251, Batch 1000/1000: LR=4.32e-04, Loss=5.38e-02 BER=2.11e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 251 Train Time 25.31789255142212s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 252, Batch 1000/1000: LR=4.31e-04, Loss=5.35e-02 BER=2.10e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 252 Train Time 25.3282208442688s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.51it/s]Training epoch 253, Batch 1000/1000: LR=4.31e-04, Loss=5.31e-02 BER=2.08e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 253 Train Time 25.313135862350464s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.36it/s]Training epoch 254, Batch 1000/1000: LR=4.30e-04, Loss=5.33e-02 BER=2.10e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 254 Train Time 25.346246004104614s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.08it/s]Training epoch 255, Batch 1000/1000: LR=4.30e-04, Loss=5.31e-02 BER=2.08e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 255 Train Time 25.362070560455322s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.38it/s]Training epoch 256, Batch 1000/1000: LR=4.29e-04, Loss=5.36e-02 BER=2.10e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 256 Train Time 25.318477153778076s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.45it/s]Training epoch 257, Batch 1000/1000: LR=4.28e-04, Loss=5.28e-02 BER=2.07e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 257 Train Time 25.335669994354248s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.54it/s]Training epoch 258, Batch 1000/1000: LR=4.28e-04, Loss=5.33e-02 BER=2.10e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 258 Train Time 25.27556538581848s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.45it/s]Training epoch 259, Batch 1000/1000: LR=4.27e-04, Loss=5.32e-02 BER=2.09e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 259 Train Time 25.290827751159668s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.39it/s]Training epoch 260, Batch 1000/1000: LR=4.27e-04, Loss=5.29e-02 BER=2.09e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 260 Train Time 25.28013324737549s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "Test EbN0=0, BER=1.58e-01 -ln(BER)=1.84e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Test EbN0=1, BER=1.14e-01 -ln(BER)=2.17e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Test EbN0=2, BER=6.73e-02 -ln(BER)=2.70e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.92it/s]\n",
      "Test EbN0=3, BER=3.66e-02 -ln(BER)=3.31e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Test EbN0=4, BER=1.34e-02 -ln(BER)=4.31e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Test EbN0=5, BER=3.87e-03 -ln(BER)=5.56e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
      "Test EbN0=6, BER=1.99e-04 -ln(BER)=8.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "Test EbN0=7, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.41it/s]Training epoch 261, Batch 1000/1000: LR=4.26e-04, Loss=5.33e-02 BER=2.09e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 261 Train Time 25.299511671066284s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 262, Batch 1000/1000: LR=4.26e-04, Loss=5.37e-02 BER=2.10e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 262 Train Time 25.325201511383057s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.40it/s]Training epoch 263, Batch 1000/1000: LR=4.25e-04, Loss=5.31e-02 BER=2.09e-02 FER=2.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 263 Train Time 25.303354740142822s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 264, Batch 1000/1000: LR=4.25e-04, Loss=5.35e-02 BER=2.11e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 264 Train Time 25.338545083999634s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.43it/s]Training epoch 265, Batch 1000/1000: LR=4.24e-04, Loss=5.33e-02 BER=2.10e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 265 Train Time 25.349757194519043s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.51it/s]Training epoch 266, Batch 1000/1000: LR=4.23e-04, Loss=5.32e-02 BER=2.10e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 266 Train Time 25.265994548797607s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 267, Batch 1000/1000: LR=4.23e-04, Loss=5.31e-02 BER=2.09e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 267 Train Time 25.33388113975525s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.70it/s]Training epoch 268, Batch 1000/1000: LR=4.22e-04, Loss=5.33e-02 BER=2.10e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 268 Train Time 25.325940132141113s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 269, Batch 1000/1000: LR=4.22e-04, Loss=5.36e-02 BER=2.11e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 269 Train Time 25.2952823638916s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.47it/s]Training epoch 270, Batch 1000/1000: LR=4.21e-04, Loss=5.27e-02 BER=2.07e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 270 Train Time 25.293245553970337s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
      "Test EbN0=0, BER=1.50e-01 -ln(BER)=1.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Test EbN0=1, BER=1.06e-01 -ln(BER)=2.24e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n",
      "Test EbN0=2, BER=7.08e-02 -ln(BER)=2.65e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "Test EbN0=3, BER=4.04e-02 -ln(BER)=3.21e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Test EbN0=4, BER=1.27e-02 -ln(BER)=4.36e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]\n",
      "Test EbN0=5, BER=3.91e-03 -ln(BER)=5.55e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]\n",
      "Test EbN0=6, BER=6.38e-04 -ln(BER)=7.36e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.49it/s]Training epoch 271, Batch 1000/1000: LR=4.21e-04, Loss=5.26e-02 BER=2.07e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 271 Train Time 25.361505031585693s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.32it/s]Training epoch 272, Batch 1000/1000: LR=4.20e-04, Loss=5.35e-02 BER=2.11e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.61it/s]\n",
      "Epoch 272 Train Time 25.245896577835083s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.38it/s]Training epoch 273, Batch 1000/1000: LR=4.19e-04, Loss=5.35e-02 BER=2.11e-02 FER=2.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.65it/s]\n",
      "Epoch 273 Train Time 25.222371339797974s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 274, Batch 1000/1000: LR=4.19e-04, Loss=5.33e-02 BER=2.09e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 274 Train Time 25.309903621673584s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.41it/s]Training epoch 275, Batch 1000/1000: LR=4.18e-04, Loss=5.37e-02 BER=2.11e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 275 Train Time 25.31903386116028s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.42it/s]Training epoch 276, Batch 1000/1000: LR=4.18e-04, Loss=5.33e-02 BER=2.10e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 276 Train Time 25.310884714126587s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.53it/s]Training epoch 277, Batch 1000/1000: LR=4.17e-04, Loss=5.34e-02 BER=2.10e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 277 Train Time 25.26245903968811s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.50it/s]Training epoch 278, Batch 1000/1000: LR=4.17e-04, Loss=5.32e-02 BER=2.09e-02 FER=2.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 278 Train Time 25.316965103149414s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.35it/s]Training epoch 279, Batch 1000/1000: LR=4.16e-04, Loss=5.70e-02 BER=2.12e-02 FER=2.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 279 Train Time 25.317516326904297s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.13it/s]Training epoch 280, Batch 1000/1000: LR=4.15e-04, Loss=5.32e-02 BER=2.09e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 280 Train Time 25.383187532424927s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Test EbN0=0, BER=1.55e-01 -ln(BER)=1.86e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "Test EbN0=1, BER=1.15e-01 -ln(BER)=2.16e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Test EbN0=2, BER=6.87e-02 -ln(BER)=2.68e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Test EbN0=3, BER=3.23e-02 -ln(BER)=3.43e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n",
      "Test EbN0=4, BER=1.31e-02 -ln(BER)=4.33e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.56it/s]\n",
      "Test EbN0=5, BER=3.23e-03 -ln(BER)=5.74e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Test EbN0=6, BER=5.58e-04 -ln(BER)=7.49e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n",
      "Test EbN0=7, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.37it/s]Training epoch 281, Batch 1000/1000: LR=4.15e-04, Loss=5.29e-02 BER=2.08e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 281 Train Time 25.354907751083374s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.48it/s]Training epoch 282, Batch 1000/1000: LR=4.14e-04, Loss=5.30e-02 BER=2.08e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 282 Train Time 25.33154535293579s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 283, Batch 1000/1000: LR=4.14e-04, Loss=5.22e-02 BER=2.06e-02 FER=2.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 283 Train Time 25.271244049072266s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.37it/s]Training epoch 284, Batch 1000/1000: LR=4.13e-04, Loss=5.32e-02 BER=2.11e-02 FER=2.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 284 Train Time 25.33787250518799s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.38it/s]Training epoch 285, Batch 1000/1000: LR=4.12e-04, Loss=5.29e-02 BER=2.08e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 285 Train Time 25.31752038002014s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.25it/s]Training epoch 286, Batch 1000/1000: LR=4.12e-04, Loss=5.35e-02 BER=2.09e-02 FER=2.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 286 Train Time 25.317155838012695s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.08it/s]Training epoch 287, Batch 1000/1000: LR=4.11e-04, Loss=5.25e-02 BER=2.06e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 287 Train Time 25.297725200653076s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.38it/s]Training epoch 288, Batch 1000/1000: LR=4.11e-04, Loss=5.25e-02 BER=2.07e-02 FER=2.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 288 Train Time 25.304726362228394s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 289, Batch 1000/1000: LR=4.10e-04, Loss=5.27e-02 BER=2.07e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 289 Train Time 25.346816062927246s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 290, Batch 1000/1000: LR=4.09e-04, Loss=5.27e-02 BER=2.07e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 290 Train Time 25.353063583374023s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s]\n",
      "Test EbN0=0, BER=1.49e-01 -ln(BER)=1.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Test EbN0=1, BER=1.15e-01 -ln(BER)=2.17e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "Test EbN0=2, BER=6.70e-02 -ln(BER)=2.70e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Test EbN0=3, BER=3.50e-02 -ln(BER)=3.35e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Test EbN0=4, BER=1.31e-02 -ln(BER)=4.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "Test EbN0=5, BER=3.35e-03 -ln(BER)=5.70e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Test EbN0=6, BER=6.78e-04 -ln(BER)=7.30e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.25it/s]\n",
      "Test EbN0=7, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.39it/s]Training epoch 291, Batch 1000/1000: LR=4.09e-04, Loss=5.23e-02 BER=2.06e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 291 Train Time 25.306602716445923s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.28it/s]Training epoch 292, Batch 1000/1000: LR=4.08e-04, Loss=5.27e-02 BER=2.08e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 292 Train Time 25.305036067962646s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.43it/s]Training epoch 293, Batch 1000/1000: LR=4.08e-04, Loss=5.25e-02 BER=2.06e-02 FER=2.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 293 Train Time 25.29365587234497s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 294, Batch 1000/1000: LR=4.07e-04, Loss=5.27e-02 BER=2.08e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 294 Train Time 25.25855541229248s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.60it/s]Training epoch 295, Batch 1000/1000: LR=4.06e-04, Loss=5.26e-02 BER=2.08e-02 FER=2.50e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 295 Train Time 25.31428098678589s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.61it/s]Training epoch 296, Batch 1000/1000: LR=4.06e-04, Loss=5.27e-02 BER=2.08e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 296 Train Time 25.26942539215088s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.32it/s]Training epoch 297, Batch 1000/1000: LR=4.05e-04, Loss=5.22e-02 BER=2.06e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 297 Train Time 25.30638575553894s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.52it/s]Training epoch 298, Batch 1000/1000: LR=4.04e-04, Loss=5.26e-02 BER=2.07e-02 FER=2.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 298 Train Time 25.32061767578125s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.51it/s]Training epoch 299, Batch 1000/1000: LR=4.04e-04, Loss=5.28e-02 BER=2.08e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 299 Train Time 25.267645120620728s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 300, Batch 1000/1000: LR=4.03e-04, Loss=5.25e-02 BER=2.06e-02 FER=2.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 300 Train Time 25.297664880752563s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Test EbN0=0, BER=1.49e-01 -ln(BER)=1.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "Test EbN0=1, BER=1.10e-01 -ln(BER)=2.21e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Test EbN0=2, BER=6.82e-02 -ln(BER)=2.69e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\n",
      "Test EbN0=3, BER=3.49e-02 -ln(BER)=3.36e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\n",
      "Test EbN0=4, BER=1.21e-02 -ln(BER)=4.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Test EbN0=5, BER=2.95e-03 -ln(BER)=5.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Test EbN0=6, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Test EbN0=7, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.11it/s]Training epoch 301, Batch 1000/1000: LR=4.03e-04, Loss=5.19e-02 BER=2.04e-02 FER=2.48e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 301 Train Time 25.358102560043335s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.36it/s]Training epoch 302, Batch 1000/1000: LR=4.02e-04, Loss=5.29e-02 BER=2.08e-02 FER=2.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 302 Train Time 25.326608419418335s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.43it/s]Training epoch 303, Batch 1000/1000: LR=4.01e-04, Loss=5.28e-02 BER=2.08e-02 FER=2.53e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 303 Train Time 25.296239614486694s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.23it/s]Training epoch 304, Batch 1000/1000: LR=4.01e-04, Loss=5.25e-02 BER=2.07e-02 FER=2.52e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 304 Train Time 25.341336011886597s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.52it/s]Training epoch 305, Batch 1000/1000: LR=4.00e-04, Loss=5.28e-02 BER=2.09e-02 FER=2.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 305 Train Time 25.340485095977783s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.05it/s]Training epoch 306, Batch 1000/1000: LR=3.99e-04, Loss=2.12e-01 BER=3.77e-02 FER=4.37e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 306 Train Time 25.293840646743774s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 307, Batch 1000/1000: LR=3.99e-04, Loss=2.07e-01 BER=4.40e-02 FER=4.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 307 Train Time 25.280545711517334s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.39it/s]Training epoch 308, Batch 1000/1000: LR=3.98e-04, Loss=1.75e-01 BER=4.29e-02 FER=4.76e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 308 Train Time 25.296480655670166s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.58it/s]Training epoch 309, Batch 1000/1000: LR=3.98e-04, Loss=1.57e-01 BER=4.17e-02 FER=4.70e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.62it/s]\n",
      "Epoch 309 Train Time 25.242316246032715s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.51it/s]Training epoch 310, Batch 1000/1000: LR=3.97e-04, Loss=1.43e-01 BER=4.06e-02 FER=4.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 310 Train Time 25.331809759140015s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Test EbN0=0, BER=1.80e-01 -ln(BER)=1.71e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Test EbN0=1, BER=1.43e-01 -ln(BER)=1.94e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Test EbN0=2, BER=1.03e-01 -ln(BER)=2.27e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Test EbN0=3, BER=6.58e-02 -ln(BER)=2.72e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Test EbN0=4, BER=4.15e-02 -ln(BER)=3.18e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Test EbN0=5, BER=1.70e-02 -ln(BER)=4.08e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Test EbN0=6, BER=6.30e-03 -ln(BER)=5.07e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Test EbN0=7, BER=1.47e-03 -ln(BER)=6.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n",
      "Test EbN0=8, BER=1.59e-04 -ln(BER)=8.74e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.85it/s]Training epoch 311, Batch 1000/1000: LR=3.96e-04, Loss=1.34e-01 BER=3.99e-02 FER=4.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 311 Train Time 25.33635139465332s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.56it/s]Training epoch 312, Batch 1000/1000: LR=3.96e-04, Loss=1.26e-01 BER=3.96e-02 FER=4.73e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n",
      "Epoch 312 Train Time 25.27118229866028s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.38it/s]Training epoch 313, Batch 1000/1000: LR=3.95e-04, Loss=1.15e-01 BER=3.77e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 313 Train Time 25.28001093864441s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.30it/s]Training epoch 314, Batch 1000/1000: LR=3.94e-04, Loss=1.06e-01 BER=3.58e-02 FER=4.50e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 314 Train Time 25.32252073287964s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.73it/s]Training epoch 315, Batch 1000/1000: LR=3.94e-04, Loss=1.02e-01 BER=3.58e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 315 Train Time 25.29420828819275s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 316, Batch 1000/1000: LR=3.93e-04, Loss=1.00e-01 BER=3.57e-02 FER=4.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 316 Train Time 25.29752254486084s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.58it/s]Training epoch 317, Batch 1000/1000: LR=3.92e-04, Loss=9.83e-02 BER=3.53e-02 FER=4.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 317 Train Time 25.311033725738525s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.51it/s]Training epoch 318, Batch 1000/1000: LR=3.92e-04, Loss=9.71e-02 BER=3.48e-02 FER=4.68e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.62it/s]\n",
      "Epoch 318 Train Time 25.239121198654175s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.84it/s]Training epoch 319, Batch 1000/1000: LR=3.91e-04, Loss=9.58e-02 BER=3.46e-02 FER=4.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 319 Train Time 25.26520013809204s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.55it/s]Training epoch 320, Batch 1000/1000: LR=3.91e-04, Loss=9.46e-02 BER=3.43e-02 FER=4.66e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 320 Train Time 25.328299522399902s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n",
      "Test EbN0=0, BER=1.66e-01 -ln(BER)=1.79e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "Test EbN0=1, BER=1.31e-01 -ln(BER)=2.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Test EbN0=2, BER=8.86e-02 -ln(BER)=2.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "Test EbN0=3, BER=5.86e-02 -ln(BER)=2.84e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Test EbN0=4, BER=3.23e-02 -ln(BER)=3.43e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "Test EbN0=5, BER=1.63e-02 -ln(BER)=4.11e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n",
      "Test EbN0=6, BER=5.34e-03 -ln(BER)=5.23e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "Test EbN0=7, BER=1.24e-03 -ln(BER)=6.70e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Test EbN0=8, BER=1.20e-04 -ln(BER)=9.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.49it/s]Training epoch 321, Batch 1000/1000: LR=3.90e-04, Loss=9.43e-02 BER=3.43e-02 FER=4.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 321 Train Time 25.303149938583374s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.09it/s]Training epoch 322, Batch 1000/1000: LR=3.89e-04, Loss=9.43e-02 BER=3.43e-02 FER=4.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 322 Train Time 25.284381866455078s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.42it/s]Training epoch 323, Batch 1000/1000: LR=3.89e-04, Loss=9.38e-02 BER=3.42e-02 FER=4.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 323 Train Time 25.29198408126831s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.16it/s]Training epoch 324, Batch 1000/1000: LR=3.88e-04, Loss=9.49e-02 BER=3.46e-02 FER=4.72e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 324 Train Time 25.277915000915527s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.17it/s]Training epoch 325, Batch 1000/1000: LR=3.87e-04, Loss=9.34e-02 BER=3.39e-02 FER=4.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 325 Train Time 25.35732078552246s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.10it/s]Training epoch 326, Batch 1000/1000: LR=3.87e-04, Loss=9.34e-02 BER=3.40e-02 FER=4.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 326 Train Time 25.30288863182068s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 327, Batch 1000/1000: LR=3.86e-04, Loss=9.34e-02 BER=3.40e-02 FER=4.66e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.53it/s]\n",
      "Epoch 327 Train Time 25.298607349395752s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.95it/s]Training epoch 328, Batch 1000/1000: LR=3.85e-04, Loss=9.28e-02 BER=3.38e-02 FER=4.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 328 Train Time 25.33407759666443s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.47it/s]Training epoch 329, Batch 1000/1000: LR=3.85e-04, Loss=9.33e-02 BER=3.41e-02 FER=4.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 329 Train Time 25.344581842422485s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.39it/s]Training epoch 330, Batch 1000/1000: LR=3.84e-04, Loss=9.18e-02 BER=3.35e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 330 Train Time 25.288761615753174s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.67it/s]\n",
      "Test EbN0=0, BER=1.69e-01 -ln(BER)=1.78e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Test EbN0=1, BER=1.34e-01 -ln(BER)=2.01e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\n",
      "Test EbN0=2, BER=9.28e-02 -ln(BER)=2.38e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.49it/s]\n",
      "Test EbN0=3, BER=5.50e-02 -ln(BER)=2.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.74it/s]\n",
      "Test EbN0=4, BER=3.26e-02 -ln(BER)=3.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\n",
      "Test EbN0=5, BER=1.50e-02 -ln(BER)=4.20e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.41it/s]\n",
      "Test EbN0=6, BER=4.46e-03 -ln(BER)=5.41e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Test EbN0=7, BER=1.59e-03 -ln(BER)=6.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.50it/s]\n",
      "Test EbN0=8, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  5.69it/s]\n",
      "Test EbN0=9, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.59it/s]Training epoch 331, Batch 1000/1000: LR=3.83e-04, Loss=9.23e-02 BER=3.37e-02 FER=4.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.41it/s]\n",
      "Epoch 331 Train Time 25.376263856887817s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.88it/s]Training epoch 332, Batch 1000/1000: LR=3.83e-04, Loss=9.15e-02 BER=3.35e-02 FER=4.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 332 Train Time 25.348368406295776s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.11it/s]Training epoch 333, Batch 1000/1000: LR=3.82e-04, Loss=9.16e-02 BER=3.35e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 333 Train Time 25.307180166244507s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.28it/s]Training epoch 334, Batch 1000/1000: LR=3.81e-04, Loss=9.19e-02 BER=3.36e-02 FER=4.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 334 Train Time 25.333394765853882s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.26it/s]Training epoch 335, Batch 1000/1000: LR=3.81e-04, Loss=9.20e-02 BER=3.36e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 335 Train Time 25.33507013320923s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 336, Batch 1000/1000: LR=3.80e-04, Loss=9.20e-02 BER=3.36e-02 FER=4.63e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 336 Train Time 25.311951637268066s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 337, Batch 1000/1000: LR=3.79e-04, Loss=9.22e-02 BER=3.36e-02 FER=4.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 337 Train Time 25.34488558769226s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.32it/s]Training epoch 338, Batch 1000/1000: LR=3.79e-04, Loss=9.16e-02 BER=3.35e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 338 Train Time 25.350545406341553s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.46it/s]Training epoch 339, Batch 1000/1000: LR=3.78e-04, Loss=9.10e-02 BER=3.33e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 339 Train Time 25.28372836112976s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.34it/s]Training epoch 340, Batch 1000/1000: LR=3.77e-04, Loss=9.02e-02 BER=3.31e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 340 Train Time 25.351760625839233s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Test EbN0=0, BER=1.63e-01 -ln(BER)=1.81e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "Test EbN0=1, BER=1.21e-01 -ln(BER)=2.11e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Test EbN0=2, BER=9.59e-02 -ln(BER)=2.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Test EbN0=3, BER=5.99e-02 -ln(BER)=2.81e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.03it/s]\n",
      "Test EbN0=4, BER=3.08e-02 -ln(BER)=3.48e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Test EbN0=5, BER=1.18e-02 -ln(BER)=4.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Test EbN0=6, BER=4.46e-03 -ln(BER)=5.41e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.89it/s]\n",
      "Test EbN0=7, BER=1.20e-03 -ln(BER)=6.73e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n",
      "Test EbN0=8, BER=2.39e-04 -ln(BER)=8.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Test EbN0=9, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.30it/s]Training epoch 341, Batch 1000/1000: LR=3.77e-04, Loss=8.92e-02 BER=3.26e-02 FER=4.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 341 Train Time 25.2897469997406s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.04it/s]Training epoch 342, Batch 1000/1000: LR=3.76e-04, Loss=9.01e-02 BER=3.31e-02 FER=4.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 342 Train Time 25.361271858215332s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.59it/s]Training epoch 343, Batch 1000/1000: LR=3.75e-04, Loss=8.99e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 343 Train Time 25.307294130325317s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.42it/s]Training epoch 344, Batch 1000/1000: LR=3.75e-04, Loss=8.94e-02 BER=3.28e-02 FER=4.51e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 344 Train Time 25.336928844451904s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 345, Batch 1000/1000: LR=3.74e-04, Loss=8.94e-02 BER=3.29e-02 FER=4.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 345 Train Time 25.327642917633057s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.52it/s]Training epoch 346, Batch 1000/1000: LR=3.73e-04, Loss=8.96e-02 BER=3.28e-02 FER=4.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 346 Train Time 25.36478543281555s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.23it/s]Training epoch 347, Batch 1000/1000: LR=3.72e-04, Loss=8.96e-02 BER=3.28e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.34it/s]\n",
      "Epoch 347 Train Time 25.417826652526855s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.53it/s]Training epoch 348, Batch 1000/1000: LR=3.72e-04, Loss=9.03e-02 BER=3.32e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 348 Train Time 25.311239004135132s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 349, Batch 1000/1000: LR=3.71e-04, Loss=8.99e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.39it/s]\n",
      "Epoch 349 Train Time 25.385322093963623s\n",
      "\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.42it/s]Training epoch 350, Batch 1000/1000: LR=3.70e-04, Loss=9.09e-02 BER=3.34e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.19it/s]\n",
      "Epoch 350 Train Time 25.520137071609497s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n",
      "Test EbN0=0, BER=1.64e-01 -ln(BER)=1.81e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Test EbN0=1, BER=1.31e-01 -ln(BER)=2.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Test EbN0=2, BER=9.61e-02 -ln(BER)=2.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Test EbN0=3, BER=5.81e-02 -ln(BER)=2.85e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n",
      "Test EbN0=4, BER=3.07e-02 -ln(BER)=3.48e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Test EbN0=5, BER=1.36e-02 -ln(BER)=4.30e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "Test EbN0=6, BER=3.63e-03 -ln(BER)=5.62e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Test EbN0=7, BER=1.32e-03 -ln(BER)=6.63e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "Test EbN0=8, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Test EbN0=9, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 351, Batch 1000/1000: LR=3.70e-04, Loss=9.07e-02 BER=3.34e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.28it/s]\n",
      "Epoch 351 Train Time 25.4590003490448s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.31it/s]Training epoch 352, Batch 1000/1000: LR=3.69e-04, Loss=9.05e-02 BER=3.33e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.33it/s]\n",
      "Epoch 352 Train Time 25.42652916908264s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.14it/s]Training epoch 353, Batch 1000/1000: LR=3.68e-04, Loss=9.03e-02 BER=3.30e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.28it/s]\n",
      "Epoch 353 Train Time 25.456729650497437s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.13it/s]Training epoch 354, Batch 1000/1000: LR=3.68e-04, Loss=9.09e-02 BER=3.34e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.34it/s]\n",
      "Epoch 354 Train Time 25.420873880386353s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 355, Batch 1000/1000: LR=3.67e-04, Loss=9.03e-02 BER=3.32e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.31it/s]\n",
      "Epoch 355 Train Time 25.441177129745483s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.24it/s]Training epoch 356, Batch 1000/1000: LR=3.66e-04, Loss=9.05e-02 BER=3.32e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 356 Train Time 25.38104820251465s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.91it/s]Training epoch 357, Batch 1000/1000: LR=3.66e-04, Loss=9.14e-02 BER=3.36e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.59it/s]\n",
      "Epoch 357 Train Time 25.2607638835907s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.30it/s]Training epoch 358, Batch 1000/1000: LR=3.65e-04, Loss=8.99e-02 BER=3.31e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 358 Train Time 25.37950110435486s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.32it/s]Training epoch 359, Batch 1000/1000: LR=3.64e-04, Loss=9.10e-02 BER=3.34e-02 FER=4.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 359 Train Time 25.335437059402466s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.33it/s]Training epoch 360, Batch 1000/1000: LR=3.63e-04, Loss=9.32e-02 BER=3.39e-02 FER=4.71e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.39it/s]\n",
      "Epoch 360 Train Time 25.3891339302063s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Test EbN0=0, BER=1.67e-01 -ln(BER)=1.79e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "Test EbN0=1, BER=1.23e-01 -ln(BER)=2.10e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=2, BER=9.43e-02 -ln(BER)=2.36e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
      "Test EbN0=3, BER=5.52e-02 -ln(BER)=2.90e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Test EbN0=4, BER=3.42e-02 -ln(BER)=3.37e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Test EbN0=5, BER=1.35e-02 -ln(BER)=4.30e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n",
      "Test EbN0=6, BER=3.95e-03 -ln(BER)=5.54e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n",
      "Test EbN0=7, BER=8.37e-04 -ln(BER)=7.09e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Test EbN0=8, BER=1.99e-04 -ln(BER)=8.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.49it/s]Training epoch 361, Batch 1000/1000: LR=3.63e-04, Loss=9.24e-02 BER=3.38e-02 FER=4.65e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.37it/s]\n",
      "Epoch 361 Train Time 25.401171445846558s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.53it/s]Training epoch 362, Batch 1000/1000: LR=3.62e-04, Loss=9.11e-02 BER=3.34e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.39it/s]\n",
      "Epoch 362 Train Time 25.385905265808105s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.27it/s]Training epoch 363, Batch 1000/1000: LR=3.61e-04, Loss=9.20e-02 BER=3.36e-02 FER=4.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.35it/s]\n",
      "Epoch 363 Train Time 25.414255142211914s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.77it/s]Training epoch 364, Batch 1000/1000: LR=3.61e-04, Loss=9.09e-02 BER=3.32e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.42it/s]\n",
      "Epoch 364 Train Time 25.36823582649231s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.12it/s]Training epoch 365, Batch 1000/1000: LR=3.60e-04, Loss=9.06e-02 BER=3.32e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.39it/s]\n",
      "Epoch 365 Train Time 25.386351585388184s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.25it/s]Training epoch 366, Batch 1000/1000: LR=3.59e-04, Loss=9.03e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.34it/s]\n",
      "Epoch 366 Train Time 25.41974711418152s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.47it/s]Training epoch 367, Batch 1000/1000: LR=3.59e-04, Loss=9.00e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.44it/s]\n",
      "Epoch 367 Train Time 25.35747981071472s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 368, Batch 1000/1000: LR=3.58e-04, Loss=9.00e-02 BER=3.31e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 368 Train Time 25.359352827072144s\n",
      "\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.21it/s]Training epoch 369, Batch 1000/1000: LR=3.57e-04, Loss=8.93e-02 BER=3.28e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.14it/s]\n",
      "Epoch 369 Train Time 25.552207946777344s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.85it/s]Training epoch 370, Batch 1000/1000: LR=3.56e-04, Loss=9.00e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.26it/s]\n",
      "Epoch 370 Train Time 25.472925186157227s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Test EbN0=0, BER=1.60e-01 -ln(BER)=1.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
      "Test EbN0=1, BER=1.31e-01 -ln(BER)=2.04e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Test EbN0=2, BER=8.84e-02 -ln(BER)=2.43e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]\n",
      "Test EbN0=3, BER=5.56e-02 -ln(BER)=2.89e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Test EbN0=4, BER=2.91e-02 -ln(BER)=3.54e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Test EbN0=5, BER=1.35e-02 -ln(BER)=4.31e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Test EbN0=6, BER=4.23e-03 -ln(BER)=5.47e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.45it/s]\n",
      "Test EbN0=7, BER=1.04e-03 -ln(BER)=6.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
      "Test EbN0=8, BER=1.20e-04 -ln(BER)=9.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "Test EbN0=9, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.39it/s]Training epoch 371, Batch 1000/1000: LR=3.56e-04, Loss=8.97e-02 BER=3.28e-02 FER=4.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.34it/s]\n",
      "Epoch 371 Train Time 25.422966718673706s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.20it/s]Training epoch 372, Batch 1000/1000: LR=3.55e-04, Loss=8.99e-02 BER=3.30e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 372 Train Time 25.344709157943726s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 38.55it/s]Training epoch 373, Batch 1000/1000: LR=3.54e-04, Loss=9.00e-02 BER=3.30e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 373 Train Time 25.33815050125122s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.18it/s]Training epoch 374, Batch 1000/1000: LR=3.54e-04, Loss=8.93e-02 BER=3.27e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.20it/s]\n",
      "Epoch 374 Train Time 25.510899782180786s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.39it/s]Training epoch 375, Batch 1000/1000: LR=3.53e-04, Loss=9.06e-02 BER=3.32e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.25it/s]\n",
      "Epoch 375 Train Time 25.476195096969604s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.28it/s]Training epoch 376, Batch 1000/1000: LR=3.52e-04, Loss=9.05e-02 BER=3.32e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.36it/s]\n",
      "Epoch 376 Train Time 25.40593123435974s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.48it/s]Training epoch 377, Batch 1000/1000: LR=3.51e-04, Loss=9.05e-02 BER=3.31e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 377 Train Time 25.33428931236267s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.04it/s]Training epoch 378, Batch 1000/1000: LR=3.51e-04, Loss=9.06e-02 BER=3.33e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.38it/s]\n",
      "Epoch 378 Train Time 25.39719843864441s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.36it/s]Training epoch 379, Batch 1000/1000: LR=3.50e-04, Loss=8.99e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.36it/s]\n",
      "Epoch 379 Train Time 25.404027462005615s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.35it/s]Training epoch 380, Batch 1000/1000: LR=3.49e-04, Loss=9.08e-02 BER=3.34e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 380 Train Time 25.329866886138916s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "Test EbN0=0, BER=1.59e-01 -ln(BER)=1.84e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n",
      "Test EbN0=1, BER=1.32e-01 -ln(BER)=2.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
      "Test EbN0=2, BER=8.77e-02 -ln(BER)=2.43e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Test EbN0=3, BER=5.65e-02 -ln(BER)=2.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Test EbN0=4, BER=3.39e-02 -ln(BER)=3.38e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Test EbN0=5, BER=1.40e-02 -ln(BER)=4.27e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n",
      "Test EbN0=6, BER=4.26e-03 -ln(BER)=5.46e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n",
      "Test EbN0=7, BER=4.38e-04 -ln(BER)=7.73e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "Test EbN0=8, BER=1.99e-04 -ln(BER)=8.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.36it/s]Training epoch 381, Batch 1000/1000: LR=3.49e-04, Loss=9.13e-02 BER=3.35e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 381 Train Time 25.337560176849365s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.39it/s]Training epoch 382, Batch 1000/1000: LR=3.48e-04, Loss=9.13e-02 BER=3.36e-02 FER=4.64e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 382 Train Time 25.314539670944214s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.36it/s]Training epoch 383, Batch 1000/1000: LR=3.47e-04, Loss=9.10e-02 BER=3.34e-02 FER=4.62e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 383 Train Time 25.34453558921814s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.48it/s]Training epoch 384, Batch 1000/1000: LR=3.46e-04, Loss=9.22e-02 BER=3.39e-02 FER=4.67e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 384 Train Time 25.32363724708557s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.63it/s]Training epoch 385, Batch 1000/1000: LR=3.46e-04, Loss=9.13e-02 BER=3.35e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 385 Train Time 25.303834915161133s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.23it/s]Training epoch 386, Batch 1000/1000: LR=3.45e-04, Loss=9.51e-02 BER=3.45e-02 FER=4.78e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.52it/s]\n",
      "Epoch 386 Train Time 25.307441234588623s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 387, Batch 1000/1000: LR=3.44e-04, Loss=9.10e-02 BER=3.34e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.56it/s]\n",
      "Epoch 387 Train Time 25.277361631393433s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.80it/s]Training epoch 388, Batch 1000/1000: LR=3.43e-04, Loss=9.02e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.36it/s]\n",
      "Epoch 388 Train Time 25.404027938842773s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.48it/s]Training epoch 389, Batch 1000/1000: LR=3.43e-04, Loss=9.10e-02 BER=3.34e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.22it/s]\n",
      "Epoch 389 Train Time 25.498519897460938s\n",
      "\n",
      "Training: 100%|█████████▉| 997/1000 [00:25<00:00, 39.54it/s]Training epoch 390, Batch 1000/1000: LR=3.42e-04, Loss=9.10e-02 BER=3.33e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.29it/s]\n",
      "Epoch 390 Train Time 25.453753232955933s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Test EbN0=0, BER=1.63e-01 -ln(BER)=1.82e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Test EbN0=1, BER=1.28e-01 -ln(BER)=2.05e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.23it/s]\n",
      "Test EbN0=2, BER=8.59e-02 -ln(BER)=2.45e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Test EbN0=3, BER=5.80e-02 -ln(BER)=2.85e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Test EbN0=4, BER=2.96e-02 -ln(BER)=3.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Test EbN0=5, BER=1.31e-02 -ln(BER)=4.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Test EbN0=6, BER=5.10e-03 -ln(BER)=5.28e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Test EbN0=7, BER=9.96e-04 -ln(BER)=6.91e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Test EbN0=8, BER=1.99e-04 -ln(BER)=8.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Test EbN0=9, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.56it/s]Training epoch 391, Batch 1000/1000: LR=3.41e-04, Loss=9.01e-02 BER=3.31e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 391 Train Time 25.31645178794861s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.78it/s]Training epoch 392, Batch 1000/1000: LR=3.41e-04, Loss=9.08e-02 BER=3.34e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.32it/s]\n",
      "Epoch 392 Train Time 25.42982578277588s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.32it/s]Training epoch 393, Batch 1000/1000: LR=3.40e-04, Loss=9.08e-02 BER=3.33e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.35it/s]\n",
      "Epoch 393 Train Time 25.413217782974243s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.01it/s]Training epoch 394, Batch 1000/1000: LR=3.39e-04, Loss=9.06e-02 BER=3.32e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.34it/s]\n",
      "Epoch 394 Train Time 25.417465209960938s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.30it/s]Training epoch 395, Batch 1000/1000: LR=3.38e-04, Loss=9.13e-02 BER=3.35e-02 FER=4.61e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.34it/s]\n",
      "Epoch 395 Train Time 25.42137622833252s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.34it/s]Training epoch 396, Batch 1000/1000: LR=3.38e-04, Loss=9.10e-02 BER=3.34e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.33it/s]\n",
      "Epoch 396 Train Time 25.423816204071045s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.85it/s]Training epoch 397, Batch 1000/1000: LR=3.37e-04, Loss=9.02e-02 BER=3.31e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 397 Train Time 25.361941814422607s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.11it/s]Training epoch 398, Batch 1000/1000: LR=3.36e-04, Loss=9.06e-02 BER=3.31e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 398 Train Time 25.316526651382446s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.31it/s]Training epoch 399, Batch 1000/1000: LR=3.35e-04, Loss=9.08e-02 BER=3.34e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 399 Train Time 25.38006591796875s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.53it/s]Training epoch 400, Batch 1000/1000: LR=3.35e-04, Loss=9.07e-02 BER=3.32e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 400 Train Time 25.2657732963562s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n",
      "Test EbN0=0, BER=1.62e-01 -ln(BER)=1.82e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Test EbN0=1, BER=1.25e-01 -ln(BER)=2.08e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Test EbN0=2, BER=8.79e-02 -ln(BER)=2.43e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n",
      "Test EbN0=3, BER=5.67e-02 -ln(BER)=2.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\n",
      "Test EbN0=4, BER=3.03e-02 -ln(BER)=3.50e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n",
      "Test EbN0=5, BER=1.46e-02 -ln(BER)=4.22e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Test EbN0=6, BER=4.26e-03 -ln(BER)=5.46e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "Test EbN0=7, BER=5.98e-04 -ln(BER)=7.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Test EbN0=8, BER=2.39e-04 -ln(BER)=8.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.19it/s]Training epoch 401, Batch 1000/1000: LR=3.34e-04, Loss=9.02e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 401 Train Time 25.33293128013611s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.05it/s]Training epoch 402, Batch 1000/1000: LR=3.33e-04, Loss=9.04e-02 BER=3.32e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.38it/s]\n",
      "Epoch 402 Train Time 25.397188901901245s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.40it/s]Training epoch 403, Batch 1000/1000: LR=3.32e-04, Loss=8.97e-02 BER=3.29e-02 FER=4.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.42it/s]\n",
      "Epoch 403 Train Time 25.366248607635498s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.54it/s]Training epoch 404, Batch 1000/1000: LR=3.32e-04, Loss=8.97e-02 BER=3.29e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.50it/s]\n",
      "Epoch 404 Train Time 25.31664514541626s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.26it/s]Training epoch 405, Batch 1000/1000: LR=3.31e-04, Loss=9.00e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.37it/s]\n",
      "Epoch 405 Train Time 25.402134656906128s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 39.52it/s]Training epoch 406, Batch 1000/1000: LR=3.30e-04, Loss=9.04e-02 BER=3.31e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.55it/s]\n",
      "Epoch 406 Train Time 25.283294916152954s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.62it/s]Training epoch 407, Batch 1000/1000: LR=3.29e-04, Loss=8.96e-02 BER=3.28e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 407 Train Time 25.266857624053955s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.56it/s]Training epoch 408, Batch 1000/1000: LR=3.29e-04, Loss=9.04e-02 BER=3.31e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.58it/s]\n",
      "Epoch 408 Train Time 25.26463770866394s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.38it/s]Training epoch 409, Batch 1000/1000: LR=3.28e-04, Loss=8.98e-02 BER=3.29e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 409 Train Time 25.35092830657959s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 37.97it/s]Training epoch 410, Batch 1000/1000: LR=3.27e-04, Loss=8.99e-02 BER=3.30e-02 FER=4.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.36it/s]\n",
      "Epoch 410 Train Time 25.407805919647217s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
      "Test EbN0=0, BER=1.60e-01 -ln(BER)=1.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Test EbN0=1, BER=1.23e-01 -ln(BER)=2.09e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Test EbN0=2, BER=9.39e-02 -ln(BER)=2.37e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Test EbN0=3, BER=5.66e-02 -ln(BER)=2.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Test EbN0=4, BER=3.24e-02 -ln(BER)=3.43e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.56it/s]\n",
      "Test EbN0=5, BER=1.44e-02 -ln(BER)=4.24e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n",
      "Test EbN0=6, BER=3.67e-03 -ln(BER)=5.61e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Test EbN0=7, BER=7.97e-04 -ln(BER)=7.13e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Test EbN0=8, BER=1.20e-04 -ln(BER)=9.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.27it/s]\n",
      "Test EbN0=9, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.21it/s]Training epoch 411, Batch 1000/1000: LR=3.27e-04, Loss=9.02e-02 BER=3.31e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.36it/s]\n",
      "Epoch 411 Train Time 25.408801078796387s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.47it/s]Training epoch 412, Batch 1000/1000: LR=3.26e-04, Loss=8.94e-02 BER=3.28e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.32it/s]\n",
      "Epoch 412 Train Time 25.43190097808838s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.34it/s]Training epoch 413, Batch 1000/1000: LR=3.25e-04, Loss=9.00e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.37it/s]\n",
      "Epoch 413 Train Time 25.399812698364258s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.95it/s]Training epoch 414, Batch 1000/1000: LR=3.24e-04, Loss=9.07e-02 BER=3.32e-02 FER=4.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.24it/s]\n",
      "Epoch 414 Train Time 25.486764907836914s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.20it/s]Training epoch 415, Batch 1000/1000: LR=3.24e-04, Loss=9.00e-02 BER=3.29e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.23it/s]\n",
      "Epoch 415 Train Time 25.48997688293457s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.58it/s]Training epoch 416, Batch 1000/1000: LR=3.23e-04, Loss=8.99e-02 BER=3.29e-02 FER=4.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.27it/s]\n",
      "Epoch 416 Train Time 25.465219736099243s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 417, Batch 1000/1000: LR=3.22e-04, Loss=9.04e-02 BER=3.32e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.29it/s]\n",
      "Epoch 417 Train Time 25.451120853424072s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.06it/s]Training epoch 418, Batch 1000/1000: LR=3.21e-04, Loss=9.02e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.34it/s]\n",
      "Epoch 418 Train Time 25.420621156692505s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.09it/s]Training epoch 419, Batch 1000/1000: LR=3.21e-04, Loss=8.96e-02 BER=3.30e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.26it/s]\n",
      "Epoch 419 Train Time 25.473145246505737s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.37it/s]Training epoch 420, Batch 1000/1000: LR=3.20e-04, Loss=9.02e-02 BER=3.31e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.45it/s]\n",
      "Epoch 420 Train Time 25.347299814224243s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n",
      "Test EbN0=0, BER=1.63e-01 -ln(BER)=1.81e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "Test EbN0=1, BER=1.23e-01 -ln(BER)=2.10e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n",
      "Test EbN0=2, BER=9.13e-02 -ln(BER)=2.39e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Test EbN0=3, BER=5.39e-02 -ln(BER)=2.92e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Test EbN0=4, BER=2.93e-02 -ln(BER)=3.53e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Test EbN0=5, BER=1.24e-02 -ln(BER)=4.39e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  9.29it/s]\n",
      "Test EbN0=6, BER=4.54e-03 -ln(BER)=5.39e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "Test EbN0=7, BER=9.57e-04 -ln(BER)=6.95e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "Test EbN0=8, BER=7.97e-05 -ln(BER)=9.44e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n",
      "Test EbN0=9, BER=3.99e-05 -ln(BER)=1.01e+01\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.46it/s]Training epoch 421, Batch 1000/1000: LR=3.19e-04, Loss=9.04e-02 BER=3.32e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.51it/s]\n",
      "Epoch 421 Train Time 25.312035083770752s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.51it/s]Training epoch 422, Batch 1000/1000: LR=3.18e-04, Loss=9.06e-02 BER=3.32e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 422 Train Time 25.34006929397583s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.56it/s]Training epoch 423, Batch 1000/1000: LR=3.17e-04, Loss=9.03e-02 BER=3.31e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 423 Train Time 25.289561986923218s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.44it/s]Training epoch 424, Batch 1000/1000: LR=3.17e-04, Loss=9.00e-02 BER=3.31e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.36it/s]\n",
      "Epoch 424 Train Time 25.405995845794678s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.73it/s]Training epoch 425, Batch 1000/1000: LR=3.16e-04, Loss=9.03e-02 BER=3.32e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.25it/s]\n",
      "Epoch 425 Train Time 25.48035430908203s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.21it/s]Training epoch 426, Batch 1000/1000: LR=3.15e-04, Loss=9.00e-02 BER=3.30e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.23it/s]\n",
      "Epoch 426 Train Time 25.49001455307007s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.87it/s]Training epoch 427, Batch 1000/1000: LR=3.14e-04, Loss=8.99e-02 BER=3.29e-02 FER=4.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.32it/s]\n",
      "Epoch 427 Train Time 25.434949159622192s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.98it/s]Training epoch 428, Batch 1000/1000: LR=3.14e-04, Loss=9.04e-02 BER=3.32e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.24it/s]\n",
      "Epoch 428 Train Time 25.48607587814331s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.75it/s]Training epoch 429, Batch 1000/1000: LR=3.13e-04, Loss=9.00e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.33it/s]\n",
      "Epoch 429 Train Time 25.423643350601196s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.54it/s]Training epoch 430, Batch 1000/1000: LR=3.12e-04, Loss=8.94e-02 BER=3.28e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.41it/s]\n",
      "Epoch 430 Train Time 25.372339248657227s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Test EbN0=0, BER=1.62e-01 -ln(BER)=1.82e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "Test EbN0=1, BER=1.26e-01 -ln(BER)=2.07e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n",
      "Test EbN0=2, BER=8.99e-02 -ln(BER)=2.41e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Test EbN0=3, BER=5.92e-02 -ln(BER)=2.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n",
      "Test EbN0=4, BER=2.85e-02 -ln(BER)=3.56e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Test EbN0=5, BER=1.25e-02 -ln(BER)=4.38e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "Test EbN0=6, BER=3.95e-03 -ln(BER)=5.54e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Test EbN0=7, BER=7.97e-04 -ln(BER)=7.13e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Test EbN0=8, BER=2.39e-04 -ln(BER)=8.34e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 39.39it/s]Training epoch 431, Batch 1000/1000: LR=3.11e-04, Loss=8.98e-02 BER=3.29e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.54it/s]\n",
      "Epoch 431 Train Time 25.290571928024292s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.48it/s]Training epoch 432, Batch 1000/1000: LR=3.11e-04, Loss=8.97e-02 BER=3.29e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 432 Train Time 25.32067108154297s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 433, Batch 1000/1000: LR=3.10e-04, Loss=8.95e-02 BER=3.30e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.46it/s]\n",
      "Epoch 433 Train Time 25.34053611755371s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.55it/s]Training epoch 434, Batch 1000/1000: LR=3.09e-04, Loss=8.97e-02 BER=3.29e-02 FER=4.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.42it/s]\n",
      "Epoch 434 Train Time 25.366976022720337s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.56it/s]Training epoch 435, Batch 1000/1000: LR=3.08e-04, Loss=8.97e-02 BER=3.30e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.49it/s]\n",
      "Epoch 435 Train Time 25.321451663970947s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.44it/s]Training epoch 436, Batch 1000/1000: LR=3.08e-04, Loss=9.03e-02 BER=3.31e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.48it/s]\n",
      "Epoch 436 Train Time 25.329843759536743s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.24it/s]Training epoch 437, Batch 1000/1000: LR=3.07e-04, Loss=9.06e-02 BER=3.33e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.41it/s]\n",
      "Epoch 437 Train Time 25.375952005386353s\n",
      "\n",
      "Training: 100%|█████████▉| 996/1000 [00:25<00:00, 38.88it/s]Training epoch 438, Batch 1000/1000: LR=3.06e-04, Loss=8.97e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.47it/s]\n",
      "Epoch 438 Train Time 25.336923360824585s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.41it/s]Training epoch 439, Batch 1000/1000: LR=3.05e-04, Loss=8.99e-02 BER=3.31e-02 FER=4.59e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.39it/s]\n",
      "Epoch 439 Train Time 25.390572786331177s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [00:25<00:00, 38.98it/s]Training epoch 440, Batch 1000/1000: LR=3.05e-04, Loss=8.98e-02 BER=3.31e-02 FER=4.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.28it/s]\n",
      "Epoch 440 Train Time 25.45583724975586s\n",
      "\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Test EbN0=0, BER=1.62e-01 -ln(BER)=1.82e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]\n",
      "Test EbN0=1, BER=1.31e-01 -ln(BER)=2.03e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.89it/s]\n",
      "Test EbN0=2, BER=9.28e-02 -ln(BER)=2.38e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Test EbN0=3, BER=5.93e-02 -ln(BER)=2.83e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Test EbN0=4, BER=2.99e-02 -ln(BER)=3.51e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.29it/s]\n",
      "Test EbN0=5, BER=1.38e-02 -ln(BER)=4.29e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.35it/s]\n",
      "Test EbN0=6, BER=4.58e-03 -ln(BER)=5.39e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "Test EbN0=7, BER=9.57e-04 -ln(BER)=6.95e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Test EbN0=8, BER=1.99e-04 -ln(BER)=8.52e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.50it/s]Training epoch 441, Batch 1000/1000: LR=3.04e-04, Loss=8.99e-02 BER=3.31e-02 FER=4.54e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 441 Train Time 25.36374306678772s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.16it/s]Training epoch 442, Batch 1000/1000: LR=3.03e-04, Loss=9.03e-02 BER=3.32e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.37it/s]\n",
      "Epoch 442 Train Time 25.4034903049469s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 38.67it/s]Training epoch 443, Batch 1000/1000: LR=3.02e-04, Loss=8.96e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.40it/s]\n",
      "Epoch 443 Train Time 25.379953384399414s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.63it/s]Training epoch 444, Batch 1000/1000: LR=3.01e-04, Loss=8.99e-02 BER=3.32e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.29it/s]\n",
      "Epoch 444 Train Time 25.452553272247314s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.49it/s]Training epoch 445, Batch 1000/1000: LR=3.01e-04, Loss=8.91e-02 BER=3.29e-02 FER=4.56e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.37it/s]\n",
      "Epoch 445 Train Time 25.399762868881226s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.12it/s]Training epoch 446, Batch 1000/1000: LR=3.00e-04, Loss=8.99e-02 BER=3.31e-02 FER=4.58e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.43it/s]\n",
      "Epoch 446 Train Time 25.361480474472046s\n",
      "\n",
      "Training: 100%|█████████▉| 998/1000 [00:25<00:00, 39.45it/s]Training epoch 447, Batch 1000/1000: LR=2.99e-04, Loss=8.94e-02 BER=3.30e-02 FER=4.57e-01\n",
      "Training: 100%|██████████| 1000/1000 [00:25<00:00, 39.38it/s]\n",
      "Epoch 447 Train Time 25.393089532852173s\n",
      "\n",
      "Training:  49%|████▊     | 487/1000 [00:12<00:12, 39.87it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, LR, config: Config):\n",
    "    model.train()\n",
    "    cum_loss = cum_ber = cum_fer = cum_samples = cum_loss = 0.\n",
    "    t = time.time()\n",
    "    batch_idx = 0\n",
    "    for m, x, z, y, magnitude, syndrome in tqdm(train_loader, position=0, leave=True, desc=\"Training\"):\n",
    "        z_mul = (y * bin_to_sign(x)) # x = 1, y = -1 => z_mul = -1\n",
    "        z_pred = model(magnitude.to(device), syndrome.to(device))\n",
    "        loss, x_pred = model.loss(z_pred, z_mul.to(device), y.to(device))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.gradient_clipping)\n",
    "        optimizer.step()\n",
    "        ###\n",
    "        ber = BER(x_pred, x.to(device))\n",
    "        fer = FER(x_pred, x.to(device))\n",
    "\n",
    "        cum_loss += loss.item() * x.shape[0]\n",
    "        cum_ber += ber * x.shape[0]\n",
    "        cum_fer += fer * x.shape[0]\n",
    "        cum_samples += x.shape[0]\n",
    "        if batch_idx == len(train_loader) - 1:\n",
    "            logging.info(\n",
    "                f'Training epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}: LR={LR:.2e}, Loss={cum_loss / cum_samples:.2e} BER={cum_ber / cum_samples:.2e} FER={cum_fer / cum_samples:.2e}')\n",
    "        batch_idx += 1\n",
    "    logging.info(f'Epoch {epoch} Train Time {time.time() - t}s\\n')\n",
    "    return cum_loss / cum_samples, cum_ber / cum_samples, cum_fer / cum_samples\n",
    "\n",
    "def test(model, device, test_loader_list, EbNo_range_test, min_FER=100):\n",
    "    model.eval()\n",
    "    t = time.time()\n",
    "    with torch.no_grad():\n",
    "        for ii, test_loader in enumerate(test_loader_list):\n",
    "            test_loss = test_ber = test_fer = cum_count = 0.\n",
    "            for m, x, z, y, magnitude, syndrome in tqdm(test_loader, position=0, leave=True, desc=\"Testing\"):\n",
    "                z_mul = (y * bin_to_sign(x))\n",
    "                z_pred = model(magnitude.to(device), syndrome.to(device))\n",
    "                loss, x_pred = model.loss(z_pred, z_mul.to(device), y.to(device))\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                test_ber += BER(x_pred, x.to(device))\n",
    "                test_fer += FER(x_pred, x.to(device))\n",
    "            ln_ber = -np.log(test_ber)\n",
    "            logging.info(f'Test EbN0={EbNo_range_test[ii]}, BER={test_ber:.2e} -ln(BER)={ln_ber:.2e}')\n",
    "\n",
    "def train_model(args: Config, model: torch.nn.Module):\n",
    "    code = args.code\n",
    "    initial_lr = args.warmup_lr\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    optimizer = Adam(model.parameters(), lr=args.warmup_lr)\n",
    "\n",
    "    # model.load_state_dict(torch.load(os.path.join(config.path, 'best_model')))\n",
    "    # optimizer.load_state_dict(torch.load(os.path.join(config.path, 'optimizer_checkpoint')))\n",
    "    \n",
    "\n",
    "    #################################\n",
    "    EbNo_range_test = range(0, 10)\n",
    "    EbNo_range_train = range(2, 8)\n",
    "    std_train = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_train]\n",
    "    std_test = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_test]\n",
    "    train_dataloader = DataLoader(ECC_Dataset(code, std_train, len=args.batch_size * 1000, zero_cw=False), batch_size=int(args.batch_size),\n",
    "                                  shuffle=True, num_workers=args.workers)\n",
    "    test_dataloader_list = [DataLoader(ECC_Dataset(code, [std_test[ii]], len=int(args.test_batch_size), zero_cw=False),\n",
    "                                       batch_size=int(args.test_batch_size), shuffle=False, num_workers=args.workers) for ii in range(len(std_test))]\n",
    "    #################################\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(1,10):\n",
    "        loss, ber, fer = train(model, device, train_dataloader, optimizer,\n",
    "                               epoch, LR=initial_lr, config=args)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            torch.save(model.state_dict(), os.path.join(args.path, 'best_model'))\n",
    "    test(model, device, test_dataloader_list, EbNo_range_test)\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = args.lr\n",
    "    \n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=1000, eta_min=args.eta_min)\n",
    "    # scheduler.load_state_dict(torch.load(os.path.join(config.path, 'scheduler_checkpoint')))\n",
    "\n",
    "    snr_step = 0\n",
    "    for epoch in range(10, args.epochs + 1):\n",
    "        # if best_loss < 2e-2:\n",
    "        #     print(\"snr_step: \", snr_step)\n",
    "        #     snr_step += 3\n",
    "        #     best_loss = float('inf')\n",
    "        #     EbNo_range_train = range(5-snr_step, 8)\n",
    "        #     std_train = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_train]\n",
    "        #     train_dataloader = DataLoader(ECC_Dataset(code, std_train, len=args.batch_size * 1000, zero_cw=False), batch_size=int(args.batch_size),\n",
    "        #                             shuffle=True, num_workers=args.workers)\n",
    "        loss, ber, fer = train(model, device, train_dataloader, optimizer,\n",
    "                               epoch, LR=scheduler.get_last_lr()[0], config=args)\n",
    "        scheduler.step()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            torch.save(model.state_dict(), os.path.join(args.path, 'best_model'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(args.path, 'optimizer_checkpoint'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(args.path, 'scheduler_checkpoint'))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test(model, device, test_dataloader_list, EbNo_range_test)\n",
    "    return model\n",
    "\n",
    "# with torch.amp.autocast(\"cuda\", enabled=False):\n",
    "train_model(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ECCM(config=config)\n",
    "model.load_state_dict(torch.load(os.path.join(config.path, 'best_model')))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import bin_to_sign\n",
    "import numpy as np\n",
    "code = config.code\n",
    "# EbNo_range_test = range(0, 10)\n",
    "EbNo_range_test = [2,3,4,5,6,7,8,9,10]\n",
    "std_test = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_test]\n",
    "test_dataloader_list = [DataLoader(ECC_Dataset(code, [std_test[ii]], len=int(config.test_batch_size), zero_cw=False),\n",
    "                                       batch_size=int(config.test_batch_size), shuffle=False, num_workers=config.workers) for ii in range(len(std_test))]\n",
    "model.eval()\n",
    "t = time.time()\n",
    "with torch.no_grad():\n",
    "    for ii, test_loader in enumerate(test_dataloader_list):\n",
    "        test_loss = test_ber = test_fer = cum_count = 0.\n",
    "        for m, x, z, y, magnitude, syndrome in tqdm(test_loader, position=0, leave=True, desc=\"Testing\"):\n",
    "            z_mul = (y * bin_to_sign(x))\n",
    "            z_pred = model(y.to(device), syndrome.to(device))\n",
    "            loss, x_pred = model.loss(z_pred, z_mul.to(device), y.to(device))\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            test_ber += BER(x_pred, x.to(device))\n",
    "            test_fer += FER(x_pred, x.to(device))\n",
    "            # break\n",
    "        ln_ber = -np.log(test_ber)\n",
    "        logging.info(f'Test EbN0={EbNo_range_test[ii]}, BER={test_ber:.2e} -ln(BER)={ln_ber:.2e} loss={loss:.2e}')\n",
    "        # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "z_pred = model(y.to('cuda'), syndrome.to('cuda'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(bin_to_sign(x[0]))\n",
    "plt.plot(bin_to_sign(x_pred[0].cpu().detach().numpy()))\n",
    "plt.plot(y[0])\n",
    "# plt.plot(z_mul[0])\n",
    "plt.figure()\n",
    "plt.plot(sign_to_bin(torch.sign(z_mul[0])))\n",
    "plt.plot(z_pred[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_cross_entropy(z_pred[0].cpu().detach(), sign_to_bin(torch.sign(z_mul[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = model(y.to('cuda'), syndrome.to('cuda'))\n",
    "print(\"z_mul\", z_mul)\n",
    "print(\"z_pred\", z_pred)\n",
    "loss, x_pred = model.loss(-z_pred, z_mul.to('cuda'), y.to('cuda'))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas:\n",
    "# Bi-directional\n",
    "# Load and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

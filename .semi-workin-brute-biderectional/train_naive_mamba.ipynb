{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/workspaces/MambaLinearCode\")\n",
    "os.chdir(\"/workspaces/MambaLinearCode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDPC_N49_K24.alist\n"
     ]
    }
   ],
   "source": [
    "from configuration import Code, Config\n",
    "from dataset import get_generator_and_parity\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def code_from_hint(hint,):\n",
    "    code_files = os.listdir(CODES_PATH)\n",
    "    code_files = [f for f in code_files if hint in f][0]\n",
    "    print(code_files)\n",
    "    code_n = int(code_files.split('_')[1][1:])\n",
    "    code_k = int(code_files.split('_')[-1][1:].split('.')[0])\n",
    "    code_type = code_files.split('_')[0]\n",
    "    code = Code(code_n, code_k, code_type)\n",
    "    return code\n",
    "\n",
    "OUTPUT_PATH = \".output/\"\n",
    "CODES_PATH = \"codes/\"\n",
    "example_code = code_from_hint(\"LDPC_N49_K24\")\n",
    "G,H = get_generator_and_parity(example_code, standard_form=True)\n",
    "example_code.generator_matrix = torch.from_numpy(G).transpose(0,1).long()\n",
    "example_code.pc_matrix = torch.from_numpy(H).long()\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "config = Config(\n",
    "    code=example_code,\n",
    "    d_model=32,\n",
    "    d_state=64,\n",
    "    path=OUTPUT_PATH,\n",
    "    N_dec=8,\n",
    "    warmup_lr=1.0e-4,\n",
    "    lr=1.0e-4,\n",
    "    epochs=1000\n",
    ")\n",
    "\n",
    "handlers = [\n",
    "        logging.FileHandler(os.path.join(OUTPUT_PATH, 'logging.txt')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s',\n",
    "                    handlers=handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import Mamba\n",
    "from dataset import EbN0_to_std, ECC_Dataset, train, test, sign_to_bin\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, LayerNorm\n",
    "import copy\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, config: Config, length) -> None:\n",
    "        super().__init__()\n",
    "        self.mamba = Mamba(\n",
    "            d_model=config.d_model,\n",
    "            d_state=config.d_state\n",
    "        )\n",
    "        self.norm = LayerNorm((length, config.d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o1 = self.mamba.forward(x)\n",
    "        o2 = torch.flip(self.mamba.forward(torch.flip(x,[1])),[1])\n",
    "        o = o1+o2\n",
    "        return self.norm(F.tanh(o))\n",
    "\n",
    "class ECCM(torch.nn.Module):\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.n = config.code.n\n",
    "        self.syndrom_length = config.code.pc_matrix.size(0)\n",
    "        self.src_embed = torch.nn.Parameter(torch.ones(\n",
    "            (self.n + self.syndrom_length, config.d_model)))\n",
    "        self.resize_output_dim = torch.nn.Linear(config.d_model, 1)\n",
    "        self.resize_output_length = torch.nn.Linear(self.n + self.syndrom_length, self.n)\n",
    "        self.norm_output = LayerNorm((self.n,))\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(p)\n",
    "        \n",
    "        self.mamba: ModuleList = clones(EncoderLayer(config, (self.n + self.syndrom_length)), config.N_dec)\n",
    "    \n",
    "    def forward(self, magnitude, syndrome):\n",
    "        emb = torch.cat([magnitude, syndrome], -1).unsqueeze(-1)\n",
    "        out: torch.Tensor = self.src_embed.unsqueeze(0) * emb\n",
    "        for sublayer in self.mamba:\n",
    "            out: torch.Tensor = sublayer.forward(out) # self.n+self.syndrom_length, d_model\n",
    "        \n",
    "        out: torch.Tensor = self.resize_output_length(out.swapaxes(-2,-1))\n",
    "        out: torch.Tensor = self.resize_output_dim(out.swapaxes(-2,-1))\n",
    "        out: torch.Tensor = out.squeeze(-1)\n",
    "        return self.norm_output(F.tanh(out))\n",
    "\n",
    "    def loss(self, z_pred, z2, y):\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            z_pred, sign_to_bin(torch.sign(z2)))\n",
    "        x_pred = sign_to_bin(torch.sign(-z_pred * torch.sign(y)))\n",
    "        return loss, x_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.86it/s]Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=7.65e-01 BER=3.82e-01 FER=1.00e+00\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.85it/s]\n",
      "Epoch 1 Train Time 259.67569375038147s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.85it/s]Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=6.67e-01 BER=1.21e-01 FER=1.00e+00\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.86it/s]\n",
      "Epoch 2 Train Time 259.3105311393738s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.86it/s]Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=6.02e-01 BER=1.19e-01 FER=1.00e+00\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.85it/s]\n",
      "Epoch 3 Train Time 259.4158065319061s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.87it/s]Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=5.44e-01 BER=1.18e-01 FER=1.00e+00\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.86it/s]\n",
      "Epoch 4 Train Time 259.2815029621124s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.85it/s]Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=4.85e-01 BER=1.10e-01 FER=9.99e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.85it/s]\n",
      "Epoch 5 Train Time 259.4821135997772s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.87it/s]Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=4.32e-01 BER=1.18e-01 FER=1.00e+00\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.85it/s]\n",
      "Epoch 6 Train Time 259.47377824783325s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.88it/s]Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=3.83e-01 BER=1.22e-01 FER=1.00e+00\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.85it/s]\n",
      "Epoch 7 Train Time 259.51741313934326s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.87it/s]Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=3.39e-01 BER=8.42e-02 FER=9.09e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.85it/s]\n",
      "Epoch 8 Train Time 259.47966480255127s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:19<00:00,  3.86it/s]Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=3.01e-01 BER=4.28e-02 FER=5.69e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:19<00:00,  3.85it/s]\n",
      "Epoch 9 Train Time 259.6358971595764s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:20<00:00,  3.81it/s]Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=2.68e-01 BER=4.06e-02 FER=5.55e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:20<00:00,  3.83it/s]\n",
      "Epoch 10 Train Time 260.77053904533386s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:20<00:00,  3.87it/s]Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=2.41e-01 BER=3.96e-02 FER=5.49e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:20<00:00,  3.84it/s]\n",
      "Epoch 11 Train Time 260.6279306411743s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:20<00:00,  3.73it/s]Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=2.18e-01 BER=3.85e-02 FER=5.43e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:20<00:00,  3.83it/s]\n",
      "Epoch 12 Train Time 260.89604687690735s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:20<00:00,  3.78it/s]Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=1.99e-01 BER=3.79e-02 FER=5.42e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:21<00:00,  3.83it/s]\n",
      "Epoch 13 Train Time 261.0593156814575s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:20<00:00,  3.81it/s]Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=1.84e-01 BER=3.75e-02 FER=5.37e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:20<00:00,  3.84it/s]\n",
      "Epoch 14 Train Time 260.60176491737366s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:25<00:00,  3.47it/s]Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=1.70e-01 BER=3.68e-02 FER=5.35e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:26<00:00,  3.76it/s]\n",
      "Epoch 15 Train Time 266.0996878147125s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:45<00:00,  3.50it/s]Training epoch 16, Batch 1000/1000: LR=9.99e-05, Loss=1.60e-01 BER=3.67e-02 FER=5.31e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:46<00:00,  3.50it/s]\n",
      "Epoch 16 Train Time 286.096067905426s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:46<00:00,  3.49it/s]Training epoch 17, Batch 1000/1000: LR=9.99e-05, Loss=1.50e-01 BER=3.63e-02 FER=5.28e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:46<00:00,  3.49it/s]\n",
      "Epoch 17 Train Time 286.71712040901184s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:36<00:00,  3.83it/s]Training epoch 18, Batch 1000/1000: LR=9.99e-05, Loss=1.42e-01 BER=3.59e-02 FER=5.23e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:36<00:00,  3.61it/s]\n",
      "Epoch 18 Train Time 276.6640522480011s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:32<00:00,  3.47it/s]Training epoch 19, Batch 1000/1000: LR=9.99e-05, Loss=1.34e-01 BER=3.56e-02 FER=5.18e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:33<00:00,  3.66it/s]\n",
      "Epoch 19 Train Time 273.17271065711975s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:48<00:00,  3.47it/s]Training epoch 20, Batch 1000/1000: LR=9.99e-05, Loss=1.28e-01 BER=3.55e-02 FER=5.19e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.46it/s]\n",
      "Epoch 20 Train Time 288.7622585296631s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:46<00:00,  3.48it/s]Training epoch 21, Batch 1000/1000: LR=9.99e-05, Loss=1.23e-01 BER=3.53e-02 FER=5.16e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 21 Train Time 287.161413192749s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:46<00:00,  3.50it/s]Training epoch 22, Batch 1000/1000: LR=9.99e-05, Loss=1.19e-01 BER=3.51e-02 FER=5.13e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 22 Train Time 287.2051110267639s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 23, Batch 1000/1000: LR=9.99e-05, Loss=1.15e-01 BER=3.48e-02 FER=5.10e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 23 Train Time 287.39481496810913s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 24, Batch 1000/1000: LR=9.99e-05, Loss=1.12e-01 BER=3.50e-02 FER=5.11e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 24 Train Time 287.4073178768158s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 25, Batch 1000/1000: LR=9.99e-05, Loss=1.09e-01 BER=3.51e-02 FER=5.10e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 25 Train Time 287.34886741638184s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:46<00:00,  3.47it/s]Training epoch 26, Batch 1000/1000: LR=9.98e-05, Loss=1.06e-01 BER=3.47e-02 FER=5.07e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 26 Train Time 287.18201756477356s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 27, Batch 1000/1000: LR=9.98e-05, Loss=1.04e-01 BER=3.48e-02 FER=5.07e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 27 Train Time 287.5147371292114s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 28, Batch 1000/1000: LR=9.98e-05, Loss=1.02e-01 BER=3.50e-02 FER=5.08e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 28 Train Time 288.03780722618103s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 29, Batch 1000/1000: LR=9.98e-05, Loss=1.00e-01 BER=3.49e-02 FER=5.07e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 29 Train Time 288.025666475296s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 30, Batch 1000/1000: LR=9.98e-05, Loss=9.77e-02 BER=3.46e-02 FER=5.05e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 30 Train Time 287.7794539928436s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 31, Batch 1000/1000: LR=9.98e-05, Loss=9.66e-02 BER=3.45e-02 FER=5.06e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 31 Train Time 287.92668986320496s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 32, Batch 1000/1000: LR=9.98e-05, Loss=9.50e-02 BER=3.43e-02 FER=5.03e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 32 Train Time 287.88130259513855s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 33, Batch 1000/1000: LR=9.98e-05, Loss=9.39e-02 BER=3.41e-02 FER=5.00e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 33 Train Time 287.9803144931793s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 34, Batch 1000/1000: LR=9.97e-05, Loss=9.24e-02 BER=3.36e-02 FER=4.93e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 34 Train Time 287.91173553466797s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 35, Batch 1000/1000: LR=9.97e-05, Loss=9.18e-02 BER=3.35e-02 FER=4.92e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 35 Train Time 288.01318311691284s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 36, Batch 1000/1000: LR=9.97e-05, Loss=9.17e-02 BER=3.36e-02 FER=4.92e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 36 Train Time 287.83525800704956s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 37, Batch 1000/1000: LR=9.97e-05, Loss=8.99e-02 BER=3.30e-02 FER=4.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 37 Train Time 287.95351696014404s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 38, Batch 1000/1000: LR=9.97e-05, Loss=8.96e-02 BER=3.29e-02 FER=4.78e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 38 Train Time 287.8613302707672s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 39, Batch 1000/1000: LR=9.96e-05, Loss=8.90e-02 BER=3.26e-02 FER=4.70e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 39 Train Time 288.24407052993774s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 40, Batch 1000/1000: LR=9.96e-05, Loss=8.86e-02 BER=3.24e-02 FER=4.60e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 40 Train Time 287.7774384021759s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 41, Batch 1000/1000: LR=9.96e-05, Loss=8.78e-02 BER=3.21e-02 FER=4.47e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 41 Train Time 288.0593183040619s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 42, Batch 1000/1000: LR=9.96e-05, Loss=8.72e-02 BER=3.16e-02 FER=4.33e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 42 Train Time 287.8299639225006s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 43, Batch 1000/1000: LR=9.96e-05, Loss=8.72e-02 BER=3.15e-02 FER=4.22e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 43 Train Time 287.93412613868713s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 44, Batch 1000/1000: LR=9.95e-05, Loss=8.70e-02 BER=3.13e-02 FER=4.12e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 44 Train Time 288.0468349456787s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 45, Batch 1000/1000: LR=9.95e-05, Loss=8.63e-02 BER=3.09e-02 FER=4.02e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 45 Train Time 287.95234966278076s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 46, Batch 1000/1000: LR=9.95e-05, Loss=8.55e-02 BER=3.07e-02 FER=3.96e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 46 Train Time 287.83020997047424s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 47, Batch 1000/1000: LR=9.95e-05, Loss=8.53e-02 BER=3.06e-02 FER=3.96e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 47 Train Time 287.6837682723999s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 48, Batch 1000/1000: LR=9.95e-05, Loss=8.51e-02 BER=3.06e-02 FER=3.95e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 48 Train Time 287.985937833786s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 49, Batch 1000/1000: LR=9.94e-05, Loss=8.45e-02 BER=3.05e-02 FER=3.92e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 49 Train Time 287.8851981163025s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 50, Batch 1000/1000: LR=9.94e-05, Loss=8.41e-02 BER=3.04e-02 FER=3.92e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 50 Train Time 287.8823564052582s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.45it/s]Training epoch 51, Batch 1000/1000: LR=9.94e-05, Loss=8.40e-02 BER=3.05e-02 FER=3.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 51 Train Time 287.89484572410583s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 52, Batch 1000/1000: LR=9.94e-05, Loss=8.40e-02 BER=3.06e-02 FER=3.93e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 52 Train Time 287.95372557640076s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 53, Batch 1000/1000: LR=9.93e-05, Loss=8.39e-02 BER=3.06e-02 FER=3.93e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 53 Train Time 287.99766635894775s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 54, Batch 1000/1000: LR=9.93e-05, Loss=8.29e-02 BER=3.03e-02 FER=3.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 54 Train Time 287.67891359329224s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 55, Batch 1000/1000: LR=9.93e-05, Loss=8.30e-02 BER=3.04e-02 FER=3.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 55 Train Time 287.88932061195374s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 56, Batch 1000/1000: LR=9.93e-05, Loss=8.36e-02 BER=3.06e-02 FER=3.93e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 56 Train Time 287.83603954315186s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 57, Batch 1000/1000: LR=9.92e-05, Loss=8.26e-02 BER=3.03e-02 FER=3.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 57 Train Time 288.1550576686859s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 58, Batch 1000/1000: LR=9.92e-05, Loss=8.25e-02 BER=3.03e-02 FER=3.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 58 Train Time 287.88375520706177s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 59, Batch 1000/1000: LR=9.92e-05, Loss=8.30e-02 BER=3.06e-02 FER=3.93e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 59 Train Time 287.8955669403076s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 60, Batch 1000/1000: LR=9.92e-05, Loss=8.25e-02 BER=3.03e-02 FER=3.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 60 Train Time 287.97780776023865s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 61, Batch 1000/1000: LR=9.91e-05, Loss=8.24e-02 BER=3.04e-02 FER=3.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 61 Train Time 287.7937767505646s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 62, Batch 1000/1000: LR=9.91e-05, Loss=8.19e-02 BER=3.02e-02 FER=3.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 62 Train Time 288.05833315849304s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 63, Batch 1000/1000: LR=9.91e-05, Loss=8.21e-02 BER=3.03e-02 FER=3.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 63 Train Time 287.92618918418884s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 64, Batch 1000/1000: LR=9.90e-05, Loss=8.20e-02 BER=3.03e-02 FER=3.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 64 Train Time 288.18455815315247s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 65, Batch 1000/1000: LR=9.90e-05, Loss=8.13e-02 BER=3.01e-02 FER=3.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 65 Train Time 287.83852529525757s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 66, Batch 1000/1000: LR=9.90e-05, Loss=8.18e-02 BER=3.03e-02 FER=3.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 66 Train Time 287.9804391860962s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 67, Batch 1000/1000: LR=9.89e-05, Loss=8.09e-02 BER=3.00e-02 FER=3.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 67 Train Time 287.66384625434875s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 68, Batch 1000/1000: LR=9.89e-05, Loss=8.11e-02 BER=3.00e-02 FER=3.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 68 Train Time 287.64202880859375s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 69, Batch 1000/1000: LR=9.89e-05, Loss=8.13e-02 BER=3.01e-02 FER=3.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 69 Train Time 287.76362705230713s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 70, Batch 1000/1000: LR=9.88e-05, Loss=8.15e-02 BER=3.04e-02 FER=3.91e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 70 Train Time 287.85858631134033s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.51it/s]Training epoch 71, Batch 1000/1000: LR=9.88e-05, Loss=8.06e-02 BER=3.00e-02 FER=3.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 71 Train Time 287.8659825325012s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 72, Batch 1000/1000: LR=9.88e-05, Loss=8.10e-02 BER=3.02e-02 FER=3.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 72 Train Time 287.81786251068115s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 73, Batch 1000/1000: LR=9.87e-05, Loss=8.13e-02 BER=3.04e-02 FER=3.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 73 Train Time 288.1589765548706s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 74, Batch 1000/1000: LR=9.87e-05, Loss=8.11e-02 BER=3.02e-02 FER=3.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 74 Train Time 287.92239689826965s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 75, Batch 1000/1000: LR=9.87e-05, Loss=8.09e-02 BER=3.02e-02 FER=3.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 75 Train Time 288.0770218372345s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 76, Batch 1000/1000: LR=9.86e-05, Loss=8.05e-02 BER=3.00e-02 FER=3.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 76 Train Time 287.81359124183655s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 77, Batch 1000/1000: LR=9.86e-05, Loss=8.07e-02 BER=3.02e-02 FER=3.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 77 Train Time 288.0028953552246s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.43it/s]Training epoch 78, Batch 1000/1000: LR=9.86e-05, Loss=8.04e-02 BER=3.01e-02 FER=3.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 78 Train Time 288.0910723209381s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 79, Batch 1000/1000: LR=9.85e-05, Loss=8.07e-02 BER=3.02e-02 FER=3.89e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 79 Train Time 287.76186871528625s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.45it/s]Training epoch 80, Batch 1000/1000: LR=9.85e-05, Loss=8.01e-02 BER=2.99e-02 FER=3.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 80 Train Time 287.75719380378723s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 81, Batch 1000/1000: LR=9.84e-05, Loss=8.02e-02 BER=3.00e-02 FER=3.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 81 Train Time 288.1008276939392s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 82, Batch 1000/1000: LR=9.84e-05, Loss=8.04e-02 BER=3.02e-02 FER=3.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 82 Train Time 287.63602805137634s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 83, Batch 1000/1000: LR=9.84e-05, Loss=7.97e-02 BER=2.99e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 83 Train Time 287.79857635498047s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 84, Batch 1000/1000: LR=9.83e-05, Loss=8.01e-02 BER=3.00e-02 FER=3.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 84 Train Time 287.9608952999115s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 85, Batch 1000/1000: LR=9.83e-05, Loss=8.00e-02 BER=2.99e-02 FER=3.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 85 Train Time 288.0209436416626s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.51it/s]Training epoch 86, Batch 1000/1000: LR=9.82e-05, Loss=7.97e-02 BER=2.99e-02 FER=3.85e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 86 Train Time 287.4312090873718s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 87, Batch 1000/1000: LR=9.82e-05, Loss=7.98e-02 BER=2.99e-02 FER=3.85e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 87 Train Time 287.9349002838135s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.45it/s]Training epoch 88, Batch 1000/1000: LR=9.82e-05, Loss=8.02e-02 BER=3.00e-02 FER=3.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:48<00:00,  3.47it/s]\n",
      "Epoch 88 Train Time 288.05882382392883s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 89, Batch 1000/1000: LR=9.81e-05, Loss=8.02e-02 BER=3.01e-02 FER=3.88e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 89 Train Time 287.91218519210815s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 90, Batch 1000/1000: LR=9.81e-05, Loss=7.92e-02 BER=2.96e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 90 Train Time 287.5553641319275s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 91, Batch 1000/1000: LR=9.80e-05, Loss=8.05e-02 BER=3.03e-02 FER=3.90e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 91 Train Time 287.8641893863678s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 92, Batch 1000/1000: LR=9.80e-05, Loss=7.87e-02 BER=2.95e-02 FER=3.83e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 92 Train Time 287.7652621269226s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.46it/s]Training epoch 93, Batch 1000/1000: LR=9.79e-05, Loss=7.93e-02 BER=2.98e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 93 Train Time 287.6382830142975s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 94, Batch 1000/1000: LR=9.79e-05, Loss=7.92e-02 BER=2.97e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 94 Train Time 287.89396262168884s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 95, Batch 1000/1000: LR=9.79e-05, Loss=7.93e-02 BER=2.98e-02 FER=3.85e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 95 Train Time 287.77389645576477s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 96, Batch 1000/1000: LR=9.78e-05, Loss=7.90e-02 BER=2.97e-02 FER=3.85e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 96 Train Time 287.95512795448303s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 97, Batch 1000/1000: LR=9.78e-05, Loss=7.96e-02 BER=2.99e-02 FER=3.87e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 97 Train Time 287.75494408607483s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 98, Batch 1000/1000: LR=9.77e-05, Loss=7.94e-02 BER=2.98e-02 FER=3.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 98 Train Time 287.8879466056824s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.45it/s]Training epoch 99, Batch 1000/1000: LR=9.77e-05, Loss=7.94e-02 BER=2.99e-02 FER=3.85e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 99 Train Time 287.94972252845764s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 100, Batch 1000/1000: LR=9.76e-05, Loss=7.94e-02 BER=2.99e-02 FER=3.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 100 Train Time 287.62611269950867s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 101, Batch 1000/1000: LR=9.76e-05, Loss=7.94e-02 BER=2.98e-02 FER=3.86e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 101 Train Time 287.7302293777466s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 102, Batch 1000/1000: LR=9.75e-05, Loss=7.95e-02 BER=2.99e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 102 Train Time 287.72514939308167s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.48it/s]Training epoch 103, Batch 1000/1000: LR=9.75e-05, Loss=7.85e-02 BER=2.96e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 103 Train Time 287.9017095565796s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.52it/s]Training epoch 104, Batch 1000/1000: LR=9.74e-05, Loss=7.84e-02 BER=2.95e-02 FER=3.83e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 104 Train Time 287.8180947303772s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.45it/s]Training epoch 105, Batch 1000/1000: LR=9.74e-05, Loss=7.89e-02 BER=2.96e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 105 Train Time 287.99536538124084s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 106, Batch 1000/1000: LR=9.73e-05, Loss=7.86e-02 BER=2.96e-02 FER=3.84e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 106 Train Time 287.9734604358673s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 107, Batch 1000/1000: LR=9.73e-05, Loss=7.87e-02 BER=2.96e-02 FER=3.83e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 107 Train Time 287.8848133087158s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.51it/s]Training epoch 108, Batch 1000/1000: LR=9.72e-05, Loss=7.82e-02 BER=2.94e-02 FER=3.82e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 108 Train Time 287.78271484375s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.47it/s]Training epoch 109, Batch 1000/1000: LR=9.72e-05, Loss=7.86e-02 BER=2.96e-02 FER=3.82e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.47it/s]\n",
      "Epoch 109 Train Time 287.8352241516113s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.49it/s]Training epoch 110, Batch 1000/1000: LR=9.71e-05, Loss=7.86e-02 BER=2.95e-02 FER=3.85e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 110 Train Time 287.7294325828552s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.50it/s]Training epoch 111, Batch 1000/1000: LR=9.71e-05, Loss=7.79e-02 BER=2.93e-02 FER=3.80e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 111 Train Time 287.74523305892944s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:46<00:00,  3.49it/s]Training epoch 112, Batch 1000/1000: LR=9.70e-05, Loss=7.84e-02 BER=2.95e-02 FER=3.81e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 112 Train Time 287.04561614990234s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:41<00:00,  3.77it/s]Training epoch 113, Batch 1000/1000: LR=9.70e-05, Loss=7.83e-02 BER=2.95e-02 FER=3.82e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:41<00:00,  3.55it/s]\n",
      "Epoch 113 Train Time 281.7263171672821s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:30<00:00,  3.43it/s]Training epoch 114, Batch 1000/1000: LR=9.69e-05, Loss=7.83e-02 BER=2.94e-02 FER=3.82e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
      "Epoch 114 Train Time 271.0090796947479s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:45<00:00,  3.48it/s]Training epoch 115, Batch 1000/1000: LR=9.69e-05, Loss=7.86e-02 BER=2.96e-02 FER=3.83e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:45<00:00,  3.50it/s]\n",
      "Epoch 115 Train Time 285.98129749298096s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:49<00:00,  3.46it/s]Training epoch 116, Batch 1000/1000: LR=9.68e-05, Loss=7.83e-02 BER=2.95e-02 FER=3.82e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:49<00:00,  3.45it/s]\n",
      "Epoch 116 Train Time 289.58054304122925s\n",
      "\n",
      "Training: 100%|█████████▉| 999/1000 [04:47<00:00,  3.45it/s]Training epoch 117, Batch 1000/1000: LR=9.67e-05, Loss=7.80e-02 BER=2.93e-02 FER=3.81e-01\n",
      "Training: 100%|██████████| 1000/1000 [04:47<00:00,  3.48it/s]\n",
      "Epoch 117 Train Time 287.51074409484863s\n",
      "\n",
      "Training:  33%|███▎      | 327/1000 [01:34<03:14,  3.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m# if epoch % 200 == 0:\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;66;03m#     test(model, device, test_dataloader_list, EbNo_range_test)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 52\u001b[0m train_model(config, model)\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# scheduler.load_state_dict(torch.load(os.path.join(config.path, 'scheduler_checkpoint')))\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     loss, ber, fer \u001b[38;5;241m=\u001b[39m train(model, device, train_dataloader, optimizer,\n\u001b[1;32m     40\u001b[0m                            epoch, LR\u001b[38;5;241m=\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m], config\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     41\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "File \u001b[0;32m/workspaces/MambaLinearCode/dataset.py:160\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, LR, config)\u001b[0m\n\u001b[1;32m    158\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    159\u001b[0m batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m, x, z, y, magnitude, syndrome \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    161\u001b[0m     z_mul \u001b[38;5;241m=\u001b[39m (y \u001b[38;5;241m*\u001b[39m bin_to_sign(x))\n\u001b[1;32m    162\u001b[0m     z_pred \u001b[38;5;241m=\u001b[39m model(magnitude\u001b[38;5;241m.\u001b[39mto(device), syndrome\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:1191\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(n \u001b[38;5;241m-\u001b[39m last_print_n)\n\u001b[1;32m   1192\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[1;32m   1193\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[1;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 459\u001b[0m     fp_write(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m len_s, \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:452\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m--> 452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m    453\u001b[0m     fp_flush()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = ECCM(config=config).to(\"cuda\")\n",
    "\n",
    "def train_model(args: Config, model: torch.nn.Module):\n",
    "    code = args.code\n",
    "    initial_lr = args.warmup_lr\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    optimizer = Adam(model.parameters(), lr=args.warmup_lr)\n",
    "\n",
    "    # model.load_state_dict(torch.load(os.path.join(config.path, 'best_model')))\n",
    "    # optimizer.load_state_dict(torch.load(os.path.join(config.path, 'optimizer_checkpoint')))\n",
    "    \n",
    "\n",
    "    #################################\n",
    "    EbNo_range_test = range(4, 7)\n",
    "    EbNo_range_train = range(2, 8)\n",
    "    std_train = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_train]\n",
    "    std_test = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_test]\n",
    "    train_dataloader = DataLoader(ECC_Dataset(code, std_train, len=args.batch_size * 1000, zero_cw=True), batch_size=int(args.batch_size),\n",
    "                                  shuffle=True, num_workers=args.workers)\n",
    "    test_dataloader_list = [DataLoader(ECC_Dataset(code, [std_test[ii]], len=int(args.test_batch_size), zero_cw=False),\n",
    "                                       batch_size=int(args.test_batch_size), shuffle=False, num_workers=args.workers) for ii in range(len(std_test))]\n",
    "    #################################\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    # for epoch in range(1,3):\n",
    "    #     loss, ber, fer = train(model, device, train_dataloader, optimizer,\n",
    "    #                            epoch, LR=initial_lr, config=args)\n",
    "    #     if loss < best_loss:\n",
    "    #         best_loss = loss\n",
    "    #         torch.save(model.state_dict(), os.path.join(args.path, 'best_model'))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = args.lr\n",
    "    \n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=1000, eta_min=args.eta_min)\n",
    "    # scheduler.load_state_dict(torch.load(os.path.join(config.path, 'scheduler_checkpoint')))\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss, ber, fer = train(model, device, train_dataloader, optimizer,\n",
    "                               epoch, LR=scheduler.get_last_lr()[0], config=args)\n",
    "        scheduler.step()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            torch.save(model.state_dict(), os.path.join(args.path, 'best_model'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(args.path, 'optimizer_checkpoint'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(args.path, 'scheduler_checkpoint'))\n",
    "\n",
    "        # if epoch % 200 == 0:\n",
    "        #     test(model, device, test_dataloader_list, EbNo_range_test)\n",
    "    return model\n",
    "\n",
    "train_model(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Test EbN0=0, BER=1.54e-01 -ln(BER)=1.87e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "Test EbN0=1, BER=1.26e-01 -ln(BER)=2.07e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Test EbN0=2, BER=8.38e-02 -ln(BER)=2.48e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "Test EbN0=3, BER=5.07e-02 -ln(BER)=2.98e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "Test EbN0=4, BER=2.49e-02 -ln(BER)=3.69e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "Test EbN0=5, BER=1.20e-02 -ln(BER)=4.42e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Test EbN0=6, BER=2.23e-03 -ln(BER)=6.10e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
      "Test EbN0=7, BER=3.19e-04 -ln(BER)=8.05e+00\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "/tmp/ipykernel_47839/1941803209.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "  ln_ber = -np.log(test_ber)\n",
      "Test EbN0=8, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Test EbN0=9, BER=0.00e+00 -ln(BER)=inf\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Test EbN0=10, BER=0.00e+00 -ln(BER)=inf\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataset import bin_to_sign, BER, FER\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def test(model, device, test_loader_list, EbNo_range_test, min_FER=100):\n",
    "    model.eval()\n",
    "    t = time.time()\n",
    "    with torch.no_grad():\n",
    "        for ii, test_loader in enumerate(test_loader_list):\n",
    "            test_loss = test_ber = test_fer = cum_count = 0.\n",
    "            for m, x, z, y, magnitude, syndrome in tqdm(test_loader, position=0, leave=True, desc=\"Testing\"):\n",
    "                z_mul = -(y * bin_to_sign(x))\n",
    "                z_pred = model(magnitude.to(device), syndrome.to(device))\n",
    "                loss, x_pred = model.loss(-z_pred, z_mul.to(device), y.to(device))\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                test_ber += BER(x_pred, x.to(device))\n",
    "                test_fer += FER(x_pred, x.to(device))\n",
    "            ln_ber = -np.log(test_ber)\n",
    "            logging.info(f'Test EbN0={EbNo_range_test[ii]}, BER={test_ber:.2e} -ln(BER)={ln_ber:.2e}')\n",
    "\n",
    "def _test(config, model):\n",
    "    EbNo_range_test = range(0, 11)\n",
    "    code = config.code\n",
    "    std_test = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_test]\n",
    "    test_dataloader_list = [DataLoader(ECC_Dataset(code, [std_test[ii]], len=int(config.test_batch_size), zero_cw=False),\n",
    "                                        batch_size=int(config.test_batch_size), shuffle=False, num_workers=config.workers) for ii in range(len(std_test))]\n",
    "    test(model, 'cuda', test_dataloader_list, EbNo_range_test)\n",
    "_test(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17237/2858039931.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(config.path, 'best_model')))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ECCM(config=config)\n",
    "model.load_state_dict(torch.load(os.path.join(config.path, 'best_model')))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49]) torch.Size([1, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.1816, 1.4278, 1.2788, 1.9569, 0.7318, 0.8519, 1.9273, 1.4133, 1.3416,\n",
       "         1.2742, 0.8815, 1.4579, 0.9340, 1.2536, 2.0998, 0.4732, 1.2108, 1.6872,\n",
       "         0.1434, 1.2015, 0.3701, 0.8556, 0.6629, 0.5573, 0.9793, 1.4336, 2.2401,\n",
       "         0.4965, 1.2220, 0.4444, 0.1743, 1.7205, 0.9718, 0.6903, 0.5496, 1.5444,\n",
       "         1.5465, 1.3216, 0.9732, 1.2767, 0.9685, 1.0407, 0.3150, 0.3329, 0.4337,\n",
       "         1.2422, 0.1428, 1.3265, 1.1648]),\n",
       " tensor([ 1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.]),\n",
       " tensor([[ 7.4994,  7.1261,  7.3770,  7.3142,  7.3708,  7.4391,  7.2971,  7.3617,\n",
       "           7.3545,  7.4357,  7.3122,  7.3635,  7.4044,  7.3483,  7.2708,  7.1398,\n",
       "           7.0714,  7.4451, -5.5284,  7.4245,  7.4163,  7.3740,  7.4801,  6.5448,\n",
       "           7.4942,  6.8799,  7.3551,  7.4026,  7.2035,  7.3659, -8.4215,  7.3658,\n",
       "           6.9867,  7.0834,  7.4865,  7.3118,  7.1665,  7.3273,  7.3761,  7.3838,\n",
       "           7.3596,  7.4058,  7.4312,  7.3280,  7.3357,  7.3680,  7.2600,  7.4214,\n",
       "           7.2187]], device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[ 1.1816,  1.4278,  1.2788,  1.9569,  0.7318,  0.8519,  1.9273,  1.4133,\n",
       "           1.3416,  1.2742,  0.8815,  1.4579,  0.9340,  1.2536,  2.0998,  0.4732,\n",
       "           1.2108,  1.6872, -0.1434,  1.2015,  0.3701,  0.8556,  0.6629,  0.5573,\n",
       "           0.9793,  1.4336,  2.2401,  0.4965,  1.2220,  0.4444, -0.1743,  1.7205,\n",
       "           0.9718,  0.6903,  0.5496,  1.5444,  1.5465,  1.3216,  0.9732,  1.2767,\n",
       "           0.9685,  1.0407,  0.3150,  0.3329,  0.4337,  1.2422,  0.1428,  1.3265,\n",
       "           1.1648]]),\n",
       " (tensor(0.0007, device='cuda:0',\n",
       "         grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
       "           1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "           0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1.]], device='cuda:0',\n",
       "         grad_fn=<MulBackward0>)))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import bin_to_sign\n",
    "\n",
    "code = config.code\n",
    "EbNo_range_train = [5]\n",
    "std_train = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_train]\n",
    "m,x,z,y,mag,syn = ECC_Dataset(code, std_train, len=config.batch_size * 1000, zero_cw=False)[0]\n",
    "z_mul = (y * bin_to_sign(x))\n",
    "if len(z_mul.shape) < 2:\n",
    "    z_mul = z_mul.unsqueeze(0)\n",
    "z_pred = model(mag.to('cuda'), syn.to('cuda'))\n",
    "print(z_pred.shape, z_mul.shape)\n",
    "mag, syn, z_pred, z_mul, model.loss(-z_pred, z_mul.to('cuda'), y.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas:\n",
    "# Bi-directional\n",
    "# Load and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
